{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c7914e0-5997-4040-95a9-22564060df6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "from torch.nn.modules.utils import _pair\n",
    "import torch.nn.functional as F\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9d7d3fa3-6aa3-4bad-b2b9-b89104e5593c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, c_dim, hidden_dim, nof_kernels):\n",
    "        super().__init__()\n",
    "        self.global_pooling = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        self.to_scores = nn.Sequential(nn.Linear(c_dim, hidden_dim),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       nn.Linear(hidden_dim, nof_kernels))\n",
    "\n",
    "    def forward(self, x, temperature=1):\n",
    "        out = self.global_pooling(x)\n",
    "        scores = self.to_scores(out)\n",
    "        # scores: (batch_size , nof_kernels)\n",
    "        return F.softmax(scores / temperature, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71dfd9ec-8114-478d-9b62-f11db51d9867",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicConv2d(nn.Module):\n",
    "    def __init__(self, nof_kernels, reduce, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        \"\"\"\n",
    "        Implementation of Dynamic convolution layer\n",
    "        :param in_channels: number of input channels.\n",
    "        :param out_channels: number of output channels.\n",
    "        :param kernel_size: size of the kernel.\n",
    "        :param groups: controls the connections between inputs and outputs.\n",
    "        in_channels and out_channels must both be divisible by groups.\n",
    "        :param nof_kernels: number of kernels to use.\n",
    "        :param reduce: Refers to the size of the hidden layer in attention: hidden = in_channels // reduce\n",
    "        :param bias: If True, convolutions also have a learnable bias\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.groups = groups\n",
    "        self.conv_args = {'stride': stride, 'padding': padding, 'dilation': dilation}\n",
    "        self.nof_kernels = nof_kernels\n",
    "        self.attention = AttentionLayer(in_channels, max(1, in_channels // reduce), nof_kernels)\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        # kernels_weights: (nof_kernels, out_channels, in_channels // groups, kernel_size, kernel_size)\n",
    "        # why groups? because we want to have the same number of kernels for each group. what is the group? it is the   # number of kernels that are applied to each input channel. So, if we have 2 groups, we will have 2 kernels for each input channel.\n",
    "        self.kernels_weights = nn.Parameter(torch.Tensor(\n",
    "            nof_kernels, out_channels, in_channels // self.groups, *self.kernel_size), requires_grad=True)\n",
    "        if bias:\n",
    "            self.kernels_bias = nn.Parameter(torch.Tensor(nof_kernels, out_channels), requires_grad=True)\n",
    "        else:\n",
    "            self.register_parameter('kernels_bias', None)\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        for i_kernel in range(self.nof_kernels):\n",
    "            init.kaiming_uniform_(self.kernels_weights[i_kernel], a=math.sqrt(5))\n",
    "        if self.kernels_bias is not None:\n",
    "            bound = 1 / math.sqrt(self.kernels_weights[0, 0].numel())\n",
    "            nn.init.uniform_(self.kernels_bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x, temperature=1):\n",
    "        # x: (batch_size , in_channels , H , W)\n",
    "        batch_size = x.shape[0]\n",
    "        # alphas: (batch_size , nof_kernels)\n",
    "        alphas = self.attention(x, temperature)\n",
    "        # agg_weights: (batch_size , out_channels , in_channels // groups, kernel_size, kernel_size)\n",
    "        # sum over the kernels with the attention weights\n",
    "        agg_weights = torch.sum(\n",
    "            torch.mul(self.kernels_weights.unsqueeze(0), alphas.view(batch_size, -1, 1, 1, 1, 1)), dim=1)\n",
    "        # Group the weights for each batch to conv2 all at once\n",
    "        \n",
    "        # agg_weights: (batch_size * out_channels , in_channels // groups, kernel_size, kernel_size)\n",
    "        agg_weights = agg_weights.view(-1, *agg_weights.shape[-3:])  # filters of shape ( out_channels , in_channels groups , ùëò ùêª , ùëò ùëä ) (out_channels, groups in_channels ‚Äã ,kH,kW) \\\n",
    "        \n",
    "\n",
    "        if self.kernels_bias is not None:\n",
    "            agg_bias = torch.sum(torch.mul(self.kernels_bias.unsqueeze(0), alphas.view(batch_size, -1, 1)), dim=1)\n",
    "            agg_bias = agg_bias.view(-1)\n",
    "        else:\n",
    "            agg_bias = None\n",
    "        \n",
    "        # why view(1, -1, *x.shape[-2:])? because we want to group the input channels. So, if we have 2 groups, we will have 2 kernels for each input channel.\n",
    "        x_grouped = x.view(1, -1, *x.shape[-2:])  # (1 , batch_size*out_c , H , W)\n",
    "        #   out: (1 , batch_size*out_C , H' , W')\n",
    "        out = F.conv2d(x_grouped, agg_weights, agg_bias, groups=self.groups * batch_size,\n",
    "                       **self.conv_args)  \n",
    "        # out: (batch_size , out_channels , H' , W')\n",
    "        out = out.view(batch_size, -1, *out.shape[-2:]) \n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a6268afd-4640-49aa-9d7e-c884a0cfb9ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 131072])\n",
      "torch.Size([4, 50])\n",
      "tensor([[0.0197, 0.0196, 0.0188, 0.0191, 0.0201, 0.0197, 0.0190, 0.0196, 0.0201,\n",
      "         0.0203, 0.0198, 0.0209, 0.0196, 0.0197, 0.0201, 0.0198, 0.0197, 0.0200,\n",
      "         0.0211, 0.0210, 0.0202, 0.0196, 0.0202, 0.0188, 0.0198, 0.0196, 0.0189,\n",
      "         0.0210, 0.0204, 0.0204, 0.0206, 0.0191, 0.0198, 0.0210, 0.0195, 0.0208,\n",
      "         0.0212, 0.0191, 0.0206, 0.0201, 0.0198, 0.0199, 0.0198, 0.0198, 0.0208,\n",
      "         0.0197, 0.0201, 0.0199, 0.0209, 0.0209],\n",
      "        [0.0195, 0.0196, 0.0186, 0.0190, 0.0199, 0.0195, 0.0193, 0.0197, 0.0202,\n",
      "         0.0205, 0.0199, 0.0207, 0.0195, 0.0197, 0.0199, 0.0199, 0.0199, 0.0199,\n",
      "         0.0211, 0.0206, 0.0201, 0.0195, 0.0202, 0.0187, 0.0198, 0.0195, 0.0188,\n",
      "         0.0209, 0.0204, 0.0207, 0.0211, 0.0193, 0.0200, 0.0210, 0.0189, 0.0209,\n",
      "         0.0213, 0.0193, 0.0207, 0.0203, 0.0195, 0.0203, 0.0200, 0.0196, 0.0211,\n",
      "         0.0199, 0.0200, 0.0197, 0.0212, 0.0205],\n",
      "        [0.0198, 0.0196, 0.0188, 0.0191, 0.0202, 0.0197, 0.0194, 0.0197, 0.0203,\n",
      "         0.0205, 0.0197, 0.0208, 0.0194, 0.0197, 0.0198, 0.0202, 0.0198, 0.0199,\n",
      "         0.0211, 0.0210, 0.0203, 0.0194, 0.0199, 0.0186, 0.0198, 0.0196, 0.0186,\n",
      "         0.0207, 0.0207, 0.0204, 0.0209, 0.0192, 0.0198, 0.0209, 0.0192, 0.0210,\n",
      "         0.0214, 0.0192, 0.0206, 0.0200, 0.0196, 0.0204, 0.0198, 0.0198, 0.0208,\n",
      "         0.0198, 0.0198, 0.0198, 0.0210, 0.0206],\n",
      "        [0.0195, 0.0198, 0.0187, 0.0195, 0.0198, 0.0196, 0.0194, 0.0197, 0.0203,\n",
      "         0.0205, 0.0199, 0.0206, 0.0196, 0.0196, 0.0199, 0.0199, 0.0198, 0.0201,\n",
      "         0.0207, 0.0208, 0.0202, 0.0196, 0.0200, 0.0189, 0.0198, 0.0194, 0.0189,\n",
      "         0.0210, 0.0207, 0.0204, 0.0207, 0.0192, 0.0198, 0.0211, 0.0192, 0.0208,\n",
      "         0.0211, 0.0192, 0.0207, 0.0200, 0.0197, 0.0203, 0.0200, 0.0197, 0.0209,\n",
      "         0.0197, 0.0197, 0.0199, 0.0211, 0.0206]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "class SimpleDynamicCNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(SimpleDynamicCNN, self).__init__()\n",
    "        # in_channels, out_channels, kernel_size, num_kernels=4, stride=1, padding=0, dilation=1, groups=1, bias=True\n",
    "        #  nof_kernels, reduce, in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True\n",
    "        \n",
    "        self.layer1 = DynamicConv2d(nof_kernels=5, reduce=4, in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)\n",
    "        self.layer2 = DynamicConv2d(5, 4, 64, 128, 3, 1, 1)\n",
    "        \n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(128 * 32 * 32, 1024)\n",
    "        self.fc2 = nn.Linear(1024, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size, 3, 128, 128)\n",
    "        # x = self.layer1(x)\n",
    "        \n",
    "        x = self.pool(F.relu(self.layer1(x))) # x shape: (batch_size, 64, 64, 64)\n",
    "        x = self.pool(F.relu(self.layer2(x))) # x shape: (batch_size, 128, 32, 32)\n",
    "\n",
    "        x = x.view(x.size(0), -1)\n",
    "        print(x.shape)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.softmax(x, dim=-1)\n",
    "\n",
    "# Test the network with a random input\n",
    "model = SimpleDynamicCNN(num_classes=50)\n",
    "input_tensor = torch.randn(4, 3, 128, 128)\n",
    "output = model(input_tensor)\n",
    "print(output.shape)\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9de41b17-3610-4e09-84d7-9ad9894b8b6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_hw1",
   "language": "python",
   "name": "dl_hw1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
