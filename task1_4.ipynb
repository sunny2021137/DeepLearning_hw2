{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "_9qO7OZuDXC_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9qO7OZuDXC_",
    "outputId": "085665a1-0587-4c17-9a2a-49b5e283f319"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vg_KqDb3DXrW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vg_KqDb3DXrW",
    "outputId": "afbf9d4e-63e0-4003-9cea-7e1ad6dc0952"
   },
   "outputs": [],
   "source": [
    "!unzip gdrive/MyDrive/images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "q8ICwrp8EBTM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8ICwrp8EBTM",
    "outputId": "2e0a3568-d279-462e-8ce7-9176bc341ff4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: wandb in /home/p76121194/.local/lib/python3.6/site-packages (0.15.12)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /home/p76121194/.local/lib/python3.6/site-packages (from wandb) (1.42.0)\n",
      "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.25.1)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.6/dist-packages (from wandb) (3.7.4.3)\n",
      "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.14.0)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from wandb) (3.12)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /home/p76121194/.local/lib/python3.6/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: pathtools in /home/p76121194/.local/lib/python3.6/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: appdirs>=1.4.3 in /home/p76121194/.local/lib/python3.6/site-packages (from wandb) (1.4.4)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /home/p76121194/.local/lib/python3.6/site-packages (from wandb) (5.9.8)\n",
      "Requirement already satisfied: setproctitle in /home/p76121194/.local/lib/python3.6/site-packages (from wandb) (1.2.3)\n",
      "Requirement already satisfied: Click!=8.0.0,>=7.1 in /home/p76121194/.local/lib/python3.6/site-packages (from wandb) (8.0.4)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from wandb) (51.1.2)\n",
      "Requirement already satisfied: GitPython!=3.1.29,>=1.0.0 in /home/p76121194/.local/lib/python3.6/site-packages (from wandb) (3.1.18)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.6/dist-packages (from Click!=8.0.0,>=7.1->wandb) (3.4.0)\n",
      "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from docker-pycreds>=0.4.0->wandb) (1.15.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /home/p76121194/.local/lib/python3.6/site-packages (from GitPython!=3.1.29,>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /home/p76121194/.local/lib/python3.6/site-packages (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (2018.1.18)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/lib/python3/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/p76121194/.local/lib/python3.6/site-packages (from requests<3,>=2.0.0->wandb) (1.26.18)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata->Click!=8.0.0,>=7.1->wandb) (3.4.0)\n",
      "\u001b[33mWARNING: You are using pip version 20.3.3; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00d1bd1-e2e0-4f71-a873-abe8c8b03b1f",
   "metadata": {
    "id": "b00d1bd1-e2e0-4f71-a873-abe8c8b03b1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p76121194/miniconda3/envs/dl_hw1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from torch.nn import init\n",
    "from torch.nn.modules.utils import _pair\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cd4e335-4e9c-4259-8fb8-92c8d5b52388",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, c_dim, hidden_dim, nof_kernels):\n",
    "        super().__init__()\n",
    "        self.global_pooling = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        self.to_scores = nn.Sequential(nn.Linear(c_dim, hidden_dim),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       nn.Linear(hidden_dim, nof_kernels)\n",
    "                                      )\n",
    "\n",
    "    def forward(self, c, x, temperature=1, pr=False):\n",
    "        # out = self.global_pooling(x)\n",
    "        # print(\"c\", c)\n",
    "        scores = self.to_scores(c)\n",
    "        # print(\"score\", scores)\n",
    "        return F.softmax(scores / temperature, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02ad8f13-503d-4246-95b2-f00cf594607c",
   "metadata": {
    "id": "02ad8f13-503d-4246-95b2-f00cf594607c"
   },
   "outputs": [],
   "source": [
    "class DynamicConv2d(nn.Module):\n",
    "    def __init__(self, nof_kernels, reduce, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        \"\"\"\n",
    "        Implementation of Dynamic convolution layer\n",
    "        :param in_channels: number of input channels.\n",
    "        :param out_channels: number of output channels.\n",
    "        :param kernel_size: size of the kernel.\n",
    "        :param groups: controls the connections between inputs and outputs.\n",
    "        in_channels and out_channels must both be divisible by groups.\n",
    "        :param nof_kernels: number of kernels to use.\n",
    "        :param reduce: Refers to the size of the hidden layer in attention: hidden = in_channels // reduce\n",
    "        :param bias: If True, convolutions also have a learnable bias\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.groups = groups\n",
    "        self.conv_args = {'stride': stride, 'padding': padding, 'dilation': dilation}\n",
    "        self.nof_kernels = nof_kernels\n",
    "        self.attention = AttentionLayer(in_channels, max(1, in_channels // reduce), nof_kernels)\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        # kernels_weights: (nof_kernels, out_channels, in_channels // groups, kernel_size, kernel_size)\n",
    "        # why groups? because we want to have the same number of kernels for each group. what is the group? it is the   # number of kernels that are applied to each input channel. So, if we have 2 groups, we will have 2 kernels for each input channel.\n",
    "        self.kernels_weights = nn.Parameter(torch.Tensor(\n",
    "            nof_kernels, out_channels, in_channels // self.groups, *self.kernel_size), requires_grad=True)\n",
    "        if bias:\n",
    "            self.kernels_bias = nn.Parameter(torch.Tensor(nof_kernels, out_channels), requires_grad=True)\n",
    "        else:\n",
    "            self.register_parameter('kernels_bias', None)\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        for i_kernel in range(self.nof_kernels):\n",
    "            init.kaiming_uniform_(self.kernels_weights[i_kernel], a=math.sqrt(5))\n",
    "        if self.kernels_bias is not None:\n",
    "            bound = 1 / math.sqrt(self.kernels_weights[0, 0].numel())\n",
    "            nn.init.uniform_(self.kernels_bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x, temperature=1):\n",
    "        # x: (batch_size , in_channels , H , W)\n",
    "        batch_size = x.shape[0]\n",
    "        # alphas: (batch_size , nof_kernels)\n",
    "        alphas = self.attention(x, temperature)\n",
    "        # agg_weights: (batch_size , out_channels , in_channels // groups, kernel_size, kernel_size)\n",
    "        # sum over the kernels with the attention weights\n",
    "        agg_weights = torch.sum(\n",
    "            torch.mul(self.kernels_weights.unsqueeze(0), alphas.view(batch_size, -1, 1, 1, 1, 1)), dim=1)\n",
    "        # Group the weights for each batch to conv2 all at once\n",
    "\n",
    "        # agg_weights: (batch_size * out_channels , in_channels // groups, kernel_size, kernel_size)\n",
    "        agg_weights = agg_weights.view(-1, *agg_weights.shape[-3:])  # filters of shape ( out_channels , in_channels groups , 𝑘 𝐻 , 𝑘 𝑊 ) (out_channels, groups in_channels ​ ,kH,kW) \\\n",
    "\n",
    "\n",
    "        if self.kernels_bias is not None:\n",
    "            agg_bias = torch.sum(torch.mul(self.kernels_bias.unsqueeze(0), alphas.view(batch_size, -1, 1)), dim=1)\n",
    "            agg_bias = agg_bias.view(-1)\n",
    "        else:\n",
    "            agg_bias = None\n",
    "\n",
    "        # why view(1, -1, *x.shape[-2:])? because we want to group the input channels. So, if we have 2 groups, we will have 2 kernels for each input channel.\n",
    "        x_grouped = x.view(1, -1, *x.shape[-2:])  # (1 , batch_size*out_c , H , W)\n",
    "        #   out: (1 , batch_size*out_C , H' , W')\n",
    "        out = F.conv2d(x_grouped, agg_weights, agg_bias, groups=self.groups * batch_size,\n",
    "                       **self.conv_args)\n",
    "        # out: (batch_size , out_channels , H' , W')\n",
    "        out = out.view(batch_size, -1, *out.shape[-2:])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "082e4782-2ee6-4bc6-8c58-59b1223c8aad",
   "metadata": {
    "id": "082e4782-2ee6-4bc6-8c58-59b1223c8aad"
   },
   "outputs": [],
   "source": [
    "class DynamicCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(DynamicCNN, self).__init__()\n",
    "\n",
    "        self.dycnn_1 = DynamicConv2d(nof_kernels=4, reduce=4, in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)\n",
    "        self.dycnn_2 = DynamicConv2d(nof_kernels=4, reduce=4, in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)\n",
    "        self.dycnn_3 = DynamicConv2d(nof_kernels=4, reduce=4, in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)\n",
    "        self.dycnn_4 = DynamicConv2d(nof_kernels=4, reduce=4, in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)\n",
    "\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, x):\n",
    "        out = F.avg_pool2d(F.relu(self.dycnn_1(c, x)), 2, 2, 0)# [32, 64, 64]\n",
    "        out = F.avg_pool2d(F.relu(self.dycnn_2(c, out)), 2, 2, 0)# [64, 32, 32]\n",
    "        out = F.avg_pool2d(F.relu(self.dycnn_3(c, out)), 2, 2, 0)# [128, 16, 16]\n",
    "        out = F.avg_pool2d(F.relu(self.dycnn_4(c, out)), 2, 2, 0)# [256, 8, 8]\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b20a4b9-a156-46cd-8e23-fe2ea0d1d527",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicConv2d_2(nn.Module):\n",
    "    def __init__(self, nof_kernels, reduce, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, groups=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.groups = groups\n",
    "        self.conv_args = {'stride': stride, 'padding': padding, 'dilation': dilation}\n",
    "        self.nof_kernels = nof_kernels\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        \n",
    "        self.kernels_weights = nn.Parameter(torch.Tensor(\n",
    "            nof_kernels, out_channels, in_channels // self.groups, *self.kernel_size), requires_grad=True)\n",
    "        if bias:\n",
    "            self.kernels_bias = nn.Parameter(torch.Tensor(nof_kernels, out_channels), requires_grad=True)\n",
    "        else:\n",
    "            self.register_parameter('kernels_bias', None)\n",
    "\n",
    "        self.attention = AttentionLayer(3, max(8, in_channels // reduce), nof_kernels)\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        for i_kernel in range(self.nof_kernels):\n",
    "            init.kaiming_uniform_(self.kernels_weights[i_kernel], a=math.sqrt(5))\n",
    "        if self.kernels_bias is not None:\n",
    "            bound = 1 / math.sqrt(self.kernels_weights[0, 0].numel())\n",
    "            nn.init.uniform_(self.kernels_bias, -bound, bound)\n",
    "\n",
    "    def forward(self, c, x, temperature=1, pr=False):\n",
    "        # x: (batch_size , in_channels , H , W)\n",
    "        batch_size = x.shape[0]\n",
    "        # alphas: (batch_size , nof_kernels)\n",
    "        alphas = self.attention(c, x, temperature)\n",
    "        # alphas = F.softmax(self.to_scores(c)/ temperature, dim=-1)\n",
    "        if pr:\n",
    "            print(\"c\", c)\n",
    "            print(\"alphas：\", alphas)\n",
    "            print(\"kernels_weights\", self.kernels_weights)\n",
    "        \n",
    "        agg_weights = torch.sum(\n",
    "            torch.mul(self.kernels_weights.unsqueeze(0), alphas.view(batch_size, -1, 1, 1, 1, 1)), dim=1)\n",
    "\n",
    "        # agg_weights: (batch_size * out_channels , in_channels // groups, kernel_size, kernel_size)\n",
    "        agg_weights = agg_weights.view(-1, *agg_weights.shape[-3:])  # filters of shape ( out_channels , in_channels groups , 𝑘 𝐻 , 𝑘 𝑊 ) (out_channels, groups in_channels ​ ,kH,kW) \\\n",
    "\n",
    "\n",
    "        if self.kernels_bias is not None:\n",
    "            agg_bias = torch.sum(torch.mul(self.kernels_bias.unsqueeze(0), alphas.view(batch_size, -1, 1)), dim=1)\n",
    "            agg_bias = agg_bias.view(-1)\n",
    "        else:\n",
    "            agg_bias = None\n",
    "\n",
    "        x_grouped = x.view(1, -1, *x.shape[-2:])  # (1 , batch_size*out_c , H , W)\n",
    "        #   out: (1 , batch_size*out_C , H' , W')\n",
    "        out = F.conv2d(x_grouped, agg_weights, agg_bias, groups=self.groups * batch_size,\n",
    "                        **self.conv_args)\n",
    "        # out: (batch_size , out_channels , H' , W')\n",
    "        out = out.view(batch_size, -1, *out.shape[-2:])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba6549f4-4344-42e5-95a0-8f417bfb8724",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicCNN_2(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(DynamicCNN_2, self).__init__()\n",
    "        \n",
    "        self.global_pooling = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        \n",
    "        self.dycnn = nn.ModuleList([DynamicConv2d_2(nof_kernels=4, reduce=4, in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True),\n",
    "                    DynamicConv2d_2(nof_kernels=4, reduce=4, in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True),\n",
    "                      DynamicConv2d_2(nof_kernels=4, reduce=4, in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True),\n",
    "                      DynamicConv2d_2(nof_kernels=4, reduce=4, in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)\n",
    "                     ])\n",
    "        self.norm =  nn.ModuleList([nn.BatchNorm2d((2**i) * 32) for i in range(4)])\n",
    "        self.act = nn.ModuleList([nn.ReLU() for i in range(4)])\n",
    "        self.pool = nn.ModuleList([nn.MaxPool2d(2, 2, 0) for i in range(4)])\n",
    "\n",
    "            \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, x, temperature=1, pr=False):\n",
    "        # channel identifier\n",
    "        # x_pool = self.global_pooling(x)\n",
    "        # channel_tensor = torch.cat((x_pool, c), dim=1)\n",
    "        channel_tensor = c.float()\n",
    "        out = x\n",
    "        for i in range(4):\n",
    "            out = self.dycnn[i](channel_tensor, out, temperature=temperature, pr = pr).to(device)\n",
    "            out = self.norm[i](out)\n",
    "            out = self.act[i](out)\n",
    "            out = self.pool[i](out)\n",
    "            \n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f18b60d9-6225-409a-a985-887fe7037ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicCNN_3(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(DynamicCNN_3, self).__init__()\n",
    "        \n",
    "        self.global_pooling = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        \n",
    "        self.dycnn = nn.ModuleList([DynamicConv2d(nof_kernels=4, reduce=4, in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True),\n",
    "                    DynamicConv2d(nof_kernels=4, reduce=4, in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True),\n",
    "                      DynamicConv2d(nof_kernels=4, reduce=4, in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True),\n",
    "                      DynamicConv2d(nof_kernels=4, reduce=4, in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)\n",
    "                     ])\n",
    "        self.norm =  nn.ModuleList([nn.BatchNorm2d((2**i) * 32) for i in range(4)])\n",
    "        self.act = nn.ModuleList([nn.ReLU() for i in range(4)])\n",
    "        self.pool = nn.ModuleList([nn.MaxPool2d(2, 2, 0) for i in range(4)])\n",
    "\n",
    "            \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, x):\n",
    "        # channel identifier\n",
    "        out = x\n",
    "        for i in range(4):\n",
    "            out = self.dycnn[i](out, temperature=30).to(device)\n",
    "            out = self.norm[i](out)\n",
    "            out = self.act[i](out)\n",
    "            out = self.pool[i](out)\n",
    "            \n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8084e57-969c-4409-8584-222d77ce79d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDynamicCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(SimpleDynamicCNN, self).__init__()\n",
    "        \n",
    "        self.dycnn = DynamicConv2d(nof_kernels=4, reduce=4, in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)\n",
    "        self.cnn = nn.Sequential(\n",
    "            # [32, 128, 128]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [32, 64, 64]\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), # [64, 64, 64]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 32, 32]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 32, 32]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 16, 16]\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 16, 16]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [256, 8, 8]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, x):\n",
    "        out = self.dycnn(c, x)\n",
    "        out = self.cnn(out)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2293ba26-5c60-464c-a02d-b2ab2eaa1f6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDynamicCNN_2(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(SimpleDynamicCNN_2, self).__init__()\n",
    "        self.dycnn = DynamicConv2d_2(nof_kernels=4, reduce=4, in_channels=3, out_channels=3, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.BatchNorm2d(3),\n",
    "            nn.ReLU(),\n",
    "            # [32, 128, 128]\n",
    "            nn.Conv2d(3, 32, 3, 1, 1), \n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [32, 64, 64]\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), # [64, 64, 64]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 32, 32]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 32, 32]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 16, 16]\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 16, 16]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [256, 8, 8]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, x, temperature=1, pr=False):\n",
    "        # x_pool = self.global_pooling(x)\n",
    "        # channel_tensor = torch.cat((x_pool, c), dim=1)\n",
    "        channel_tensor = c.float()\n",
    "        out = self.dycnn(channel_tensor, x, temperature=temperature, pr=pr)\n",
    "        out = self.cnn(out)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "879c502d-50d9-4b9c-93a1-e6e7833f17a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1), # [32, 128, 128]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [32, 64, 64]\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), # [64, 64, 64]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 32, 32]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 32, 32]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 16, 16]\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 16, 16]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [256, 8, 8]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, x, temperature=1, pr=False):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f24cad6-34eb-4e48-8d2d-0c71b5eab3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN_4(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(SimpleCNN_4, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1), # [32, 128, 128]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [32, 64, 64]\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), # [64, 64, 64]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 32, 32]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 32, 32]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 16, 16]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128*16*16, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, x, temperature=1, pr=False):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e173e4e0-a822-424b-8248-c6b24ce45eab",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicCNN_4(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(DynamicCNN_4, self).__init__()\n",
    "        \n",
    "        self.global_pooling = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        \n",
    "        self.dycnn = nn.ModuleList([DynamicConv2d_2(nof_kernels=4, reduce=4, in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True),\n",
    "                    DynamicConv2d_2(nof_kernels=4, reduce=4, in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True),\n",
    "                      DynamicConv2d_2(nof_kernels=4, reduce=4, in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, dilation=1, groups=1, bias=True),\n",
    "                                   ])\n",
    "        self.norm =  nn.ModuleList([nn.BatchNorm2d((2**i) * 32) for i in range(3)])\n",
    "        self.act = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2, 0)\n",
    "\n",
    "            \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128*16*16, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, c, x, temperature=1, pr=False):\n",
    "        # channel identifier\n",
    "        # x_pool = self.global_pooling(x)\n",
    "        # channel_tensor = torch.cat((x_pool, c), dim=1)\n",
    "        channel_tensor = c.float()\n",
    "        out = x\n",
    "        for i in range(4):\n",
    "            out = self.dycnn[i](channel_tensor, out, temperature=temperature, pr = pr).to(device)\n",
    "            out = self.norm[i](out)\n",
    "            out = self.act(out)\n",
    "            out = self.pool(out)\n",
    "            \n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aca5383e-bdc2-4111-bdb1-1559a45bb702",
   "metadata": {
    "id": "aca5383e-bdc2-4111-bdb1-1559a45bb702"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, channel, x, y=None, transform=None):\n",
    "        self.channel = channel\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        C = self.channel[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return C, X, Y\n",
    "        else:\n",
    "            return C, X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6c4b9ecf-81b1-497d-83a9-fcb1b58d03d0",
   "metadata": {
    "id": "6c4b9ecf-81b1-497d-83a9-fcb1b58d03d0"
   },
   "outputs": [],
   "source": [
    "def random_channel(images, seed):\n",
    "    # 设置种子\n",
    "    random.seed(seed)\n",
    "\n",
    "    new_images = []\n",
    "    channel_dict = {0:'BGR', 1:'GR', 2:'BG', 3:'R', 4:'G', 5:'B'}\n",
    "    numbers = list(channel_dict.keys())\n",
    "    channels = []\n",
    "    for image in images:\n",
    "        channel_idx = random.choice(numbers)\n",
    "        # 修改通道\n",
    "        if channel_dict[channel_idx] == 'BGR':\n",
    "            img = image[:, :, :]\n",
    "        elif channel_dict[channel_idx] == 'GR':\n",
    "            img = image[:, :, 1:]\n",
    "        elif channel_dict[channel_idx] == 'BG':\n",
    "            img = image[:, :, :2]\n",
    "        elif channel_dict[channel_idx] == 'R':\n",
    "            img = image[:, :, 2:3]\n",
    "        elif channel_dict[channel_idx] == 'G':\n",
    "            img = image[:, :, 1:2]\n",
    "        elif channel_dict[channel_idx] == 'B':\n",
    "            img = image[:, :, 0:1]\n",
    "        new_images.append(img)\n",
    "        channels.append(channel_dict[channel_idx])\n",
    "    return channels, new_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "274ca17c-2a38-4461-b261-cdcb8c1d161e",
   "metadata": {
    "id": "274ca17c-2a38-4461-b261-cdcb8c1d161e"
   },
   "outputs": [],
   "source": [
    "def load_img(f):\n",
    "    shapes = []\n",
    "    f=open(f)\n",
    "    lines=f.readlines()\n",
    "    imgs, lab=[], []\n",
    "    for i in range(len(lines)):\n",
    "        fn, label = lines[i].split(' ')\n",
    "        im1=cv2.imread(fn)\n",
    "\n",
    "        if im1.shape[2] not in shapes:\n",
    "            shapes.append(im1.shape[2])\n",
    "        # im1=cv2.resize(im1, (img_size,img_size))\n",
    "        # im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # im1 = preprocessing(im1, op_list)\n",
    "        # vec = np.reshape(im1, [-1])\n",
    "\n",
    "        imgs.append(im1)\n",
    "        lab.append(int(label))\n",
    "    print(i)\n",
    "\n",
    "    # imgs= np.asarray(imgs, np.uint8)\n",
    "    lab= np.asarray(lab, np.uint8)\n",
    "    # print(shapes)\n",
    "    return imgs, lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f970f412-c990-49f0-9e0e-fd7df519431e",
   "metadata": {
    "id": "f970f412-c990-49f0-9e0e-fd7df519431e"
   },
   "outputs": [],
   "source": [
    "def resize_input(channels, images, img_size):\n",
    "    channel_map = {\"BGR\": torch.tensor([1,1,1]), \"BG\":torch.tensor([1,1,0]), \"GR\":torch.tensor([0,1,1]), \"B\":torch.tensor([1,0,0]), \"G\":torch.tensor([0,1,0]), \"R\":torch.tensor([0,0,1])}\n",
    "    x = np.zeros((len(images), img_size, img_size, 3), dtype=np.uint8)\n",
    "    c = np.zeros((len(images), 3),  dtype=np.uint8)\n",
    "    for i, img in enumerate(images):\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "\n",
    "        if channels[i] == \"B\" or channels[i] == \"G\" or channels[i] == \"R\": # 1維\n",
    "            img_3channel = np.stack((img,) * 3, axis=-1)\n",
    "        elif channels[i] == \"BGR\":\n",
    "            img_3channel = img\n",
    "        elif channels[i] == \"BG\":\n",
    "            # 生成第三個通道，可以根據具體需求來決定其值\n",
    "            third_channel = np.mean(img, axis=2)  # 這裡用前兩個通道的平均值作為示例\n",
    "            # 合併成三通道圖像\n",
    "            img_3channel = np.zeros((img_size, img_size, 3), dtype=img.dtype)\n",
    "            img_3channel[:, :, :2] = img  # 前兩個通道保持不變\n",
    "            img_3channel[:, :, 2] = third_channel  # 第三個通道為生成的數據\n",
    "        elif channels[i] == \"GR\":\n",
    "            # 生成第三個通道，可以根據具體需求來決定其值\n",
    "            third_channel = np.mean(img, axis=2)  # 這裡用前兩個通道的平均值作為示例\n",
    "            # 合併成三通道圖像\n",
    "            img_3channel = np.zeros((img_size, img_size, 3), dtype=img.dtype)\n",
    "            img_3channel[:, :, 1:] = img  # 後兩個通道保持不變\n",
    "            img_3channel[:, :, 0] = third_channel  # 第一個通道為生成的數據\n",
    "        else:\n",
    "            print(\"error\")\n",
    "        # 變c, w, h\n",
    "        # img_3channel = np.transpose(img_3channel, (2, 0, 1))\n",
    "        x[i, :, :, :] = img_3channel\n",
    "        c[i, :] = channel_map[channels[i]]\n",
    "    return c, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "_hI4HXjfHRKF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "_hI4HXjfHRKF",
    "outputId": "c9914f4d-1b62-45a5-d042-97da637119eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msunny2021137\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fd392f61-9289-4fe8-a30a-c037c2fb947a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "fd392f61-9289-4fe8-a30a-c037c2fb947a",
    "outputId": "f1acf56e-2a64-4db3-84aa-5cb89be3df5c"
   },
   "outputs": [],
   "source": [
    "\n",
    "    #############\n",
    "    eval_time = 1\n",
    "    num_epoch = 30\n",
    "    num_classes = 50\n",
    "    img_size = 144\n",
    "    batch_size = 128\n",
    "    lr = 0.001\n",
    "    #############\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "aNhJvgvAI1SQ",
   "metadata": {
    "id": "aNhJvgvAI1SQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63324\n",
      "449\n",
      "449\n"
     ]
    }
   ],
   "source": [
    "    x, y = load_img('train.txt')\n",
    "    vx, vy = load_img('val.txt')\n",
    "    tx, ty = load_img('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6PexMzryGbi1",
   "metadata": {
    "id": "6PexMzryGbi1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--1--\n"
     ]
    }
   ],
   "source": [
    "    print(\"--1--\")\n",
    "    c, x_new = random_channel(x, seed=42)\n",
    "    vc, vx_new = random_channel(vx, seed=42)\n",
    "    tc, tx_new = random_channel(tx, seed=42)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38694e1b-6887-419e-8cb5-be3cf43dea40",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "413a023a-0df3-4786-8b3d-e4e14a93fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, vx, tx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "FE_PFUEYGgif",
   "metadata": {
    "id": "FE_PFUEYGgif"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2--\n"
     ]
    }
   ],
   "source": [
    "    print(\"--2--\")\n",
    "    c, x_resize = resize_input(c, x_new, img_size=img_size)\n",
    "    vc, vx_resize = resize_input(vc, vx_new, img_size=img_size)\n",
    "    tc, tx_resize = resize_input(tc, tx_new, img_size=img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e11169a3-36b3-4578-9283-5f93f43b1616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0]\n",
      " [1 1 1]\n",
      " [1 1 1]\n",
      " ...\n",
      " [1 0 0]\n",
      " [1 1 0]\n",
      " [0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c5097372-b883-4461-8c9f-ea518fdb7ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--3--\n"
     ]
    }
   ],
   "source": [
    "    print(\"--3--\")\n",
    "    # training 時做 data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((144, 144)),  # 縮放\n",
    "        transforms.RandomRotation(degrees=30),  # 旋轉\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 平移\n",
    "        transforms.RandomCrop(128),  # 隨機裁剪\n",
    "        transforms.RandomHorizontalFlip(),  # 水平翻轉\n",
    "        # transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # 調整顏色\n",
    "        transforms.ToTensor(),  # 轉換為Tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 標準化\n",
    "\n",
    "    ])\n",
    "    # testing 時不需做 data augmentation\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((144, 144)),  # 縮放\n",
    "        transforms.CenterCrop(128),  # 中心裁剪\n",
    "        transforms.ToTensor(),  # 轉換為Tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 標準化\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "deaa6257-1e21-45b9-84ec-d9542b38e4ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "del x_new, vx_new, tx_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "R7rzfzd_GlSl",
   "metadata": {
    "id": "R7rzfzd_GlSl"
   },
   "outputs": [],
   "source": [
    "    train_set = ImgDataset(c, x_resize, y, train_transform)\n",
    "    val_set = ImgDataset(vc, vx_resize, vy, test_transform)\n",
    "    test_set = ImgDataset(tc, tx_resize, ty, test_transform)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6506b2fa-0a04-40fe-ae8e-939217199dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p76121194/dl_hw2/wandb/run-20240610_152810-ab1run03</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sunny2021137/task1_4/runs/ab1run03' target=\"_blank\">peachy-terrain-52</a></strong> to <a href='https://wandb.ai/sunny2021137/task1_4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sunny2021137/task1_4' target=\"_blank\">https://wandb.ai/sunny2021137/task1_4</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sunny2021137/task1_4/runs/ab1run03' target=\"_blank\">https://wandb.ai/sunny2021137/task1_4/runs/ab1run03</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_metric.Metric at 0x7f4c455d2280>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"task1_4\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"model\": \"simple_4\",\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": num_epoch,\n",
    "        \"img_size\": img_size,\n",
    "    },)\n",
    "    wandb.define_metric(\"Train/epoch\")\n",
    "    wandb.define_metric(\"Train/*\", step_metric=\"Train/epoch\")\n",
    "    wandb.define_metric(\"Val/epoch\")\n",
    "    wandb.define_metric(\"Val/*\", step_metric=\"Val/epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0SPBb0ciGqa_",
   "metadata": {
    "id": "0SPBb0ciGqa_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n",
      "1\n",
      "5\n",
      "1\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # print(\"--4--\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = torch.device(\"cpu\")\n",
    "    model = SimpleCNN_4(num_classes=num_classes).to(device)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) # optimizer 使用 Adam\n",
    "\n",
    "    temperature = 1\n",
    "    for epoch in range(num_epoch):\n",
    "        print(epoch)\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "        for i, data in enumerate(train_loader):\n",
    "            # print(data[0].shape, data[1].shape)\n",
    "            optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "            temperature = 1\n",
    "            # if epoch < 5:\n",
    "            #     temperature = 30 - 6 * epoch\n",
    "            \n",
    "            \n",
    "            train_pred = model(data[0].to(device), data[1].to(device), temperature=temperature) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "            batch_loss = loss(train_pred, data[2].to(device)) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "            batch_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
    "            optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "            train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[2].numpy())\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "        if True:\n",
    "            print(temperature)\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    pr1 = False\n",
    "                    if i == 0:\n",
    "                        pr1 = True\n",
    "                    val_pred = model(data[0].to(device), data[1].to(device), temperature=temperature, pr =pr1)\n",
    "                    batch_loss = loss(val_pred, data[2].to(device))\n",
    "\n",
    "                    val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[2].numpy())\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                #將結果 print 出來\n",
    "                # print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "                #     (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "                #      train_acc/train_set.__len__(), train_loss/train_set.__len__(), val_acc/val_set.__len__(), val_loss/val_set.__len__()))\n",
    "\n",
    "                wandb.log({\"Train/epoch\": epoch,\n",
    "                            \"Train/acc\": train_acc/train_set.__len__(),\n",
    "                           \"Train/loss\": train_loss/train_set.__len__(),\n",
    "                           \"Val/epoch\": epoch,\n",
    "                           \"Val/acc\": val_acc/val_set.__len__(),\n",
    "                           \"Val/loss\": val_loss/val_set.__len__(),\n",
    "                          })\n",
    "\n",
    "    print(\"--5--\")\n",
    "    model.eval() \n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            test_pred = model(data[0].to(device), data[1].to(device), temperature=temperature)\n",
    "            batch_loss = loss(test_pred, data[2].to(device))\n",
    "            test_acc += np.sum(np.argmax(test_pred.cpu().data.numpy(), axis=1) == data[2].numpy())\n",
    "            test_loss += batch_loss.item()\n",
    "        wandb.log({\"Test/test acc\": test_acc/test_set.__len__(),\n",
    "                  \"Test/loss\": test_loss/test_set.__len__(),})\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472cc50-0195-4286-b540-6dad3cd2aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009ab3d-721f-49b1-a82f-8ec5b78fad75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575677b3-96a3-40fc-8c72-7ff9977f1033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391925f-0521-4d18-b97e-c5e4da08e6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl_hw1",
   "language": "python",
   "name": "dl_hw1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
