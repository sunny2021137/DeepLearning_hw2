{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00d1bd1-e2e0-4f71-a873-abe8c8b03b1f",
   "metadata": {
    "id": "b00d1bd1-e2e0-4f71-a873-abe8c8b03b1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p76121194/miniconda3/envs/dl_hw1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from torch.nn import init\n",
    "from torch.nn.modules.utils import _pair\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from thop import profile, clever_format\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67dd633a-ba38-4d5b-919f-a9d740b05f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cafd4235-5d5c-4d34-be4d-aed1d42d25f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionLayer(nn.Module):\n",
    "    def __init__(self, c_dim, hidden_dim, nof_kernels, out_channel, in_channel):\n",
    "        super().__init__()\n",
    "        self.nof_kernels = nof_kernels\n",
    "        self.out_channel = out_channel\n",
    "        self.in_channel = in_channel\n",
    "        self.global_pooling = nn.Sequential(nn.AdaptiveAvgPool2d(1), nn.Flatten())\n",
    "        self.to_scores = nn.Sequential(nn.Linear(in_channel, hidden_dim),\n",
    "                                       nn.ReLU(inplace=True),\n",
    "                                       nn.Linear(hidden_dim, nof_kernels*out_channel*in_channel)\n",
    "                                      )\n",
    "\n",
    "    def forward(self, x, temperature=1):\n",
    "        out = self.global_pooling(x)\n",
    "        scores = self.to_scores(out)\n",
    "        scores = scores.reshape(x.shape[0], self.nof_kernels, self.out_channel, self.in_channel)\n",
    "        return F.softmax(scores / temperature, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b03bd3a5-273e-48d7-bc94-0e1ae51cc232",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicConv2d(nn.Module):\n",
    "    def __init__(self, nof_kernels, reduce, in_channels, out_channels, kernel_size,\n",
    "                 stride=1, padding=0, dilation=1, bias=True):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.conv_args = {'stride': stride, 'padding': padding, 'dilation': dilation}\n",
    "        self.nof_kernels = nof_kernels\n",
    "        self.kernel_size = _pair(kernel_size)\n",
    "        # kernels_weights: (nof_kernels, out_channels, in_channels, *self.kernel_size)\n",
    "        self.kernels_weights = nn.Parameter(torch.Tensor(\n",
    "            nof_kernels, out_channels, in_channels, *self.kernel_size), requires_grad=True)\n",
    "        if bias:\n",
    "            self.kernels_bias = nn.Parameter(torch.Tensor(out_channels), requires_grad=True)\n",
    "        else:\n",
    "            self.register_parameter('kernels_bias', None)\n",
    "\n",
    "        self.attention = AttentionLayer(3, max(8, in_channels // reduce), nof_kernels, out_channels, in_channels)\n",
    "        self.initialize_parameters()\n",
    "\n",
    "    def initialize_parameters(self):\n",
    "        for i_kernel in range(self.nof_kernels):\n",
    "            init.kaiming_uniform_(self.kernels_weights[i_kernel], a=math.sqrt(5))\n",
    "        if self.kernels_bias is not None:\n",
    "            bound = 1 / math.sqrt(self.kernels_weights[0, 0].numel())\n",
    "            nn.init.uniform_(self.kernels_bias, -bound, bound)\n",
    "\n",
    "    def forward(self, x, temperature=1):\n",
    "        # x: (batch_size , in_channels , H , W)\n",
    "        batch_size = x.shape[0]\n",
    "        # alphas: (batch_size , nof_kernels, self.out_channels, self.in_channels)\n",
    "        alphas = self.attention(x, temperature)\n",
    "\n",
    "        # self.kernels_weights.unsqueeze(0): (1, nof_kernels, out_channels, in_channels, self.kernel_size, self.kernel_size)\n",
    "        # alphas.view(): (batch_size , nof_kernels, self.out_channels, self.in_channels, 1, 1)\n",
    "        # agg_weights: (batch_size, self.out_channels, self.in_channels, self.kernel_size, self.kernel_size)\n",
    "        agg_weights = torch.sum(\n",
    "            torch.mul(self.kernels_weights.unsqueeze(0), alphas.view(batch_size, self.nof_kernels, self.out_channels, self.in_channels, 1, 1)), dim=1)\n",
    "\n",
    "        # agg_weights: (batch_size * out_channels , in_channels, kernel_size, kernel_size)\n",
    "        agg_weights = agg_weights.view(-1, *agg_weights.shape[-3:])\n",
    "\n",
    "\n",
    "        if self.kernels_bias is not None:\n",
    "            agg_bias = self.kernels_bias.repeat(batch_size)\n",
    "        else:\n",
    "            agg_bias = None\n",
    "\n",
    "        x_grouped = x.view(1, -1, *x.shape[-2:])  # (1 , batch_size*out_c , H , W)\n",
    "        #   out: (1 , batch_size*out_C , H' , W')\n",
    "        out = F.conv2d(x_grouped, agg_weights, agg_bias, groups=batch_size,\n",
    "                        **self.conv_args)\n",
    "        # out: (batch_size , out_channels , H' , W')\n",
    "        out = out.view(batch_size, -1, *out.shape[-2:])\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5a331090-a425-40af-848c-51af5a7d6637",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DynamicCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50, nof_kernels=4):\n",
    "        super(DynamicCNN, self).__init__()\n",
    "        self.dycnn = nn.ModuleList([\n",
    "            DynamicConv2d(nof_kernels=nof_kernels, reduce=4, in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, dilation=1, bias=True),\n",
    "                    DynamicConv2d(nof_kernels=nof_kernels, reduce=4, in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, dilation=1, bias=True),\n",
    "                      DynamicConv2d(nof_kernels=nof_kernels, reduce=4, in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, dilation=1, bias=True),\n",
    "                      DynamicConv2d(nof_kernels=nof_kernels, reduce=4, in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, dilation=1, bias=True)\n",
    "                     ])\n",
    "        self.norm =  nn.ModuleList([nn.BatchNorm2d((2**i) * 32) for i in range(4)])\n",
    "        self.act = nn.ReLU()\n",
    "        self.pool = nn.MaxPool2d(2, 2, 0)\n",
    "\n",
    "            \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, temperature=1):\n",
    "        out = x\n",
    "        for i in range(4):\n",
    "            out = self.dycnn[i](out, temperature=temperature)\n",
    "            out = self.norm[i](out)\n",
    "            out = self.act(out)\n",
    "            out = self.pool(out)\n",
    "            \n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079064bd-bea3-4ce6-ba89-0014fad6fe67",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros', device=None, dtype=None)\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1), # [32, 128, 128]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [32, 64, 64]\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), # [64, 64, 64]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 32, 32]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 32, 32]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 16, 16]\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 16, 16]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [256, 8, 8]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x, temperature=1):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aca5383e-bdc2-4111-bdb1-1559a45bb702",
   "metadata": {
    "id": "aca5383e-bdc2-4111-bdb1-1559a45bb702"
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c4b9ecf-81b1-497d-83a9-fcb1b58d03d0",
   "metadata": {
    "id": "6c4b9ecf-81b1-497d-83a9-fcb1b58d03d0"
   },
   "outputs": [],
   "source": [
    "def random_channel(images, channel_combinations):\n",
    "    channel_dict = {i:c for i,c in enumerate(channel_combinations)}\n",
    "    new_images = []\n",
    "    n = len(list(channel_dict.keys()))\n",
    "    channels = []\n",
    "    for i, image in enumerate(images):\n",
    "        channel_idx = i % n\n",
    "        # 修改通道\n",
    "        if channel_dict[channel_idx] == 'BGR':\n",
    "            img = image[:, :, :]\n",
    "        elif channel_dict[channel_idx] == 'GR':\n",
    "            img = image[:, :, 1:]\n",
    "        elif channel_dict[channel_idx] == 'BG':\n",
    "            img = image[:, :, :2]\n",
    "        elif channel_dict[channel_idx] == 'R':\n",
    "            img = image[:, :, 2:3]\n",
    "        elif channel_dict[channel_idx] == 'G':\n",
    "            img = image[:, :, 1:2]\n",
    "        elif channel_dict[channel_idx] == 'B':\n",
    "            img = image[:, :, 0:1]\n",
    "        else:\n",
    "            print(\"error\")\n",
    "        new_images.append(img)\n",
    "        channels.append(channel_dict[channel_idx])\n",
    "    return channels, new_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f28a9c5-be30-441e-abe6-8d5bf30c1a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(model_name, nof_kernels=4):\n",
    "    model = None\n",
    "    if model_name == \"DynamicCNN\":\n",
    "        model = DynamicCNN(num_classes=50, nof_kernels=nof_kernels)\n",
    "    elif model_name == \"base_model\":\n",
    "        model = SimpleCNN(num_classes=50)\n",
    "        \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274ca17c-2a38-4461-b261-cdcb8c1d161e",
   "metadata": {
    "id": "274ca17c-2a38-4461-b261-cdcb8c1d161e"
   },
   "outputs": [],
   "source": [
    "def load_img(f):\n",
    "    shapes = []\n",
    "    f=open(f)\n",
    "    lines=f.readlines()\n",
    "    imgs, lab=[], []\n",
    "    for i in range(len(lines)):\n",
    "        fn, label = lines[i].split(' ')\n",
    "        im1=cv2.imread(fn)\n",
    "\n",
    "        if im1.shape[2] not in shapes:\n",
    "            shapes.append(im1.shape[2])\n",
    "\n",
    "        imgs.append(im1)\n",
    "        lab.append(int(label))\n",
    "\n",
    "    \n",
    "    lab= np.asarray(lab, np.uint8)\n",
    "    \n",
    "    return imgs, lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b56c455f-a089-46f5-ae1b-86b24d72adf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_input(channels, images, img_size):\n",
    "    channel_map = {\"BGR\": torch.tensor([1,1,1]), \"BG\":torch.tensor([1,1,0]), \"GR\":torch.tensor([0,1,1]), \"B\":torch.tensor([1,0,0]), \"G\":torch.tensor([0,1,0]), \"R\":torch.tensor([0,0,1])}\n",
    "    x = np.zeros((len(images), img_size, img_size, 3), dtype=np.uint8)\n",
    "    c = np.zeros((len(images), 3),  dtype=np.uint8)\n",
    "    for i, img in enumerate(images):\n",
    "        # 3通道圖片\n",
    "        img_3channel = np.zeros((img_size, img_size, 3), dtype=img.dtype)\n",
    "        \n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "\n",
    "        if channels[i] == \"B\":\n",
    "            img_3channel[:, :, 0] = img \n",
    "        elif channels[i] == \"G\":\n",
    "            img_3channel[:, :, 1] = img \n",
    "        elif channels[i] == \"R\":\n",
    "            img_3channel[:, :, 2] = img \n",
    "        elif channels[i] == \"BGR\":\n",
    "            img_3channel = img\n",
    "        elif channels[i] == \"BG\":\n",
    "            # 合併成三通道圖像\n",
    "            img_3channel[:, :, :2] = img  # 前兩個通道保持不變\n",
    "        elif channels[i] == \"GR\":\n",
    "            img_3channel[:, :, 1:] = img  # 後兩個通道保持不變\n",
    "        else:\n",
    "            print(\"error\")\n",
    "        x[i, :, :, :] = img_3channel\n",
    "        c[i, :] = channel_map[channels[i]]\n",
    "    return c, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4948f163-08bc-48d3-8d20-449b23071e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_complexity(model, inputs):\n",
    "    # 计算FLOPs和参数量\n",
    "    flops, params = profile(model, inputs=(inputs,))\n",
    "    flops, params = clever_format([flops, params], \"%.3f\")\n",
    "    print(f\"FLOPs: {flops}\")\n",
    "    print(f\"Params: {params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "879b291b-2ea9-4182-9366-0b15c78819d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, eval_time, num_epoch, n_train, n_val, temperature, lr, device):\n",
    "    # print(\"--4--\")\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # model = model.to(device)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) # optimizer 使用 Adam\n",
    "\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        print(epoch)\n",
    "        \n",
    "        epoch_start_time = time.time()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "            \n",
    "            train_pred = model(data[0].to(device), temperature=temperature) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "            batch_loss = loss(train_pred, data[1].to(device)) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "            batch_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
    "            optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "            train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "        if epoch % eval_time == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    val_pred = model(data[0].to(device), temperature=temperature)\n",
    "                    batch_loss = loss(val_pred, data[1].to(device))\n",
    "\n",
    "                    val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                # 將結果 print 出來\n",
    "                print('[%03d/%03d] %2.2f sec(s) Train Acc: %3.6f Loss: %3.6f | Val Acc: %3.6f loss: %3.6f' % \\\n",
    "                    (epoch + 1, num_epoch, time.time()-epoch_start_time, \\\n",
    "                     train_acc/n_train, train_loss/n_train, val_acc/n_val, val_loss/n_val))\n",
    "\n",
    "                wandb.log({\"Train/epoch\": epoch,\n",
    "                            \"Train/acc\": train_acc/n_train,\n",
    "                           \"Train/loss\": train_loss/n_train,\n",
    "                           \"Val/epoch\": epoch,\n",
    "                           \"Val/acc\": val_acc/n_val,\n",
    "                           \"Val/loss\": val_loss/n_val,\n",
    "                          })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "398843e5-8cdb-44e7-8c10-91147241758a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, n_test, temperature, device):\n",
    "    model.eval()\n",
    "    # device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # model = model.to(device)\n",
    "    loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            test_pred = model(data[0].to(device), temperature=temperature)\n",
    "            batch_loss = loss(test_pred, data[1].to(device))\n",
    "            test_loss += batch_loss.item()\n",
    "\n",
    "            _, preds = torch.max(test_pred, 1)\n",
    "            all_labels.extend(data[1].cpu().numpy())\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        precision = precision_score(all_labels, all_preds, average='weighted')\n",
    "        recall = recall_score(all_labels, all_preds, average='weighted')\n",
    "        f1 = f1_score(all_labels, all_preds, average='weighted')\n",
    "\n",
    "        print(\"--------Test result-------------\")\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        print(f'Precision: {precision:.4f}')\n",
    "        print(f'Recall: {recall:.4f}')\n",
    "        print(f'F1-score: {f1:.4f}')\n",
    "        print(f'Loss: {test_loss//n_test:.4f}')\n",
    "        print(\"--------------------------------\")\n",
    "\n",
    "        wandb.log({\"Test/Accuracy\": accuracy,\n",
    "                   \"Test/Precision\": precision,\n",
    "                   \"Test/Recall\": recall,\n",
    "                   \"Test/F1-score\": f1,\n",
    "                  \"Test/Loss\": test_loss/n_test,})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712eff19-db2a-4a10-8906-548ec2d6242d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6662e4f1-ed9e-45ce-bdb0-ae66f253f698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nof_kernel\n",
    "def experiment_2():\n",
    "    # 超參數\n",
    "    #############\n",
    "    eval_time = 1\n",
    "    num_epoch = 50\n",
    "    num_classes = 50\n",
    "    img_size = 144\n",
    "    input_size = 128\n",
    "    batch_size = 128\n",
    "    lr = 0.001\n",
    "    model_name= 'DynamicCNN'\n",
    "    temperature = 1\n",
    "    #############\n",
    "    set_seed(42)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    x, y = load_img('train.txt')\n",
    "    vx, vy = load_img('val.txt')\n",
    "    tx, ty = load_img('test.txt')\n",
    "    print(\"--1--\")\n",
    "    # 產生隨機組合通道數的圖片\n",
    "    channel_combinations = ['BGR','GR','BG','R','G','B']\n",
    "    c, x_new = random_channel(x, channel_combinations)\n",
    "    vc, vx_new = random_channel(vx, channel_combinations)\n",
    "    tc, tx_new = random_channel(tx, channel_combinations)\n",
    "    del x, vx, tx\n",
    "    print(\"--2--\")\n",
    "    # 填補成三通道\n",
    "    c, x_resize = resize_input(c, x_new, img_size=img_size)\n",
    "    vc, vx_resize = resize_input(vc, vx_new, img_size=img_size)\n",
    "    tc, tx_resize = resize_input(tc, tx_new, img_size=img_size)\n",
    "    del x_new, vx_new, tx_new\n",
    "    print(\"--3--\")\n",
    "    # 定義transform\n",
    "    # training 時做 data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((img_size, img_size)),  # 縮放\n",
    "        transforms.RandomRotation(degrees=30),  # 旋轉\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 平移\n",
    "        transforms.RandomCrop(input_size),  # 隨機裁剪\n",
    "        transforms.RandomHorizontalFlip(),  # 水平翻轉\n",
    "        transforms.ToTensor(),  # 轉換為Tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 標準化\n",
    "\n",
    "    ])\n",
    "    # testing 時不需做 data augmentation\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((img_size, img_size)),  # 縮放\n",
    "        transforms.CenterCrop(input_size),  # 中心裁剪\n",
    "        transforms.ToTensor(),  # 轉換為Tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 標準化\n",
    "    ])\n",
    "    # data loader\n",
    "    train_set = ImgDataset(x_resize, y, train_transform)\n",
    "    val_set = ImgDataset(vx_resize, vy, test_transform)\n",
    "    test_set = ImgDataset(tx_resize, ty, test_transform)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=lambda _: np.random.seed(42), num_workers=4)\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    del x_resize, vx_resize, tx_resize\n",
    "    \n",
    "    print(\"--4--\")\n",
    "    wandb.login()\n",
    "    for nof_kernels in [1, 3, 5, 7]:\n",
    "        print(\"-----number of kernels = \"+str(nof_kernels)+\"-----\")\n",
    "        # create model\n",
    "        model = create_model(model_name, nof_kernels=nof_kernels).to(device)\n",
    "        # 測FLOPs、params\n",
    "        # 创建输入张量\n",
    "        inputs = torch.randn(1, 3, input_size, input_size)\n",
    "        calc_complexity(model, inputs.to(device))\n",
    "        \n",
    "        run = wandb.init(\n",
    "        # Set the project where this run will be logged\n",
    "        project=\"task1_experiment_1\",\n",
    "        name=model_name+\"_nof_kernels=\"+str(nof_kernels),\n",
    "        config={\n",
    "            \"model\": model_name,\n",
    "            \"learning_rate\": lr,\n",
    "            \"epochs\": num_epoch,\n",
    "            \"input_size\": input_size,\n",
    "            \"nof_kernels\": nof_kernels,\n",
    "        },)\n",
    "        wandb.define_metric(\"Train/epoch\")\n",
    "        wandb.define_metric(\"Train/*\", step_metric=\"Train/epoch\")\n",
    "        wandb.define_metric(\"Val/epoch\")\n",
    "        wandb.define_metric(\"Val/*\", step_metric=\"Val/epoch\")\n",
    "        n_train = train_set.__len__()\n",
    "        n_val = val_set.__len__()\n",
    "        n_test = test_set.__len__()\n",
    "        train(model, train_loader, val_loader, eval_time, num_epoch, n_train, n_val, temperature, lr, device)\n",
    "        test(model, test_loader, n_test, temperature, device)\n",
    "        run.finish()\n",
    "        del model\n",
    "        gc.collect()\n",
    "        \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a432d155-4d4e-4b6e-b91b-2735c305e690",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--1--\n",
      "--2--\n",
      "--3--\n",
      "--4--\n",
      "-----number of kernels = 1-----\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "FLOPs: 22.740M\n",
      "Params: 18.575M\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p76121194/dl_hw2/wandb/run-20240617_165047-jo26447k</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sunny2021137/task1_experiment_1/runs/jo26447k' target=\"_blank\">DynamicCNN_nof_kernels=1</a></strong> to <a href='https://wandb.ai/sunny2021137/task1_experiment_1' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sunny2021137/task1_experiment_1' target=\"_blank\">https://wandb.ai/sunny2021137/task1_experiment_1</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sunny2021137/task1_experiment_1/runs/jo26447k' target=\"_blank\">https://wandb.ai/sunny2021137/task1_experiment_1/runs/jo26447k</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[001/050] 108.65 sec(s) Train Acc: 0.041232 Loss: 0.030025 | Val Acc: 0.086667 loss: 0.029715\n",
      "1\n",
      "[002/050] 107.63 sec(s) Train Acc: 0.113320 Loss: 0.026247 | Val Acc: 0.106667 loss: 0.028164\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "experiment_2() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760ebff3-004b-4210-b156-510e7acaf7b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----channel_combinations =  ['BGR', 'GR', 'BG', 'R', 'G', 'B']----\n",
      "[INFO] Register count_adap_avgpool() for <class 'torch.nn.modules.pooling.AdaptiveAvgPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "FLOPs: 26.331M\n",
      "Params: 22.295M\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p76121194/dl_hw2/wandb/run-20240617_094445-wmn1p6rk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sunny2021137/task1_experiment_2/runs/wmn1p6rk' target=\"_blank\">DynamicCNN_combination= BGR+GR+BG+R+G+B</a></strong> to <a href='https://wandb.ai/sunny2021137/task1_experiment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sunny2021137/task1_experiment_2' target=\"_blank\">https://wandb.ai/sunny2021137/task1_experiment_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sunny2021137/task1_experiment_2/runs/wmn1p6rk' target=\"_blank\">https://wandb.ai/sunny2021137/task1_experiment_2/runs/wmn1p6rk</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[001/030] 109.38 sec(s) Train Acc: 0.051291 Loss: 0.029229 | Val Acc: 0.104444 loss: 0.029386\n",
      "1\n",
      "[002/030] 109.39 sec(s) Train Acc: 0.114252 Loss: 0.026001 | Val Acc: 0.122222 loss: 0.028201\n",
      "2\n",
      "[003/030] 108.91 sec(s) Train Acc: 0.146498 Loss: 0.024756 | Val Acc: 0.160000 loss: 0.026196\n",
      "3\n",
      "[004/030] 109.04 sec(s) Train Acc: 0.172791 Loss: 0.023736 | Val Acc: 0.164444 loss: 0.025917\n",
      "4\n",
      "[005/030] 108.44 sec(s) Train Acc: 0.200663 Loss: 0.022758 | Val Acc: 0.217778 loss: 0.023329\n",
      "5\n",
      "[006/030] 102.26 sec(s) Train Acc: 0.226435 Loss: 0.021815 | Val Acc: 0.240000 loss: 0.023065\n",
      "6\n",
      "[007/030] 104.21 sec(s) Train Acc: 0.248464 Loss: 0.020963 | Val Acc: 0.262222 loss: 0.022817\n",
      "7\n",
      "[008/030] 104.20 sec(s) Train Acc: 0.273241 Loss: 0.020134 | Val Acc: 0.264444 loss: 0.022768\n",
      "8\n",
      "[009/030] 104.94 sec(s) Train Acc: 0.293265 Loss: 0.019458 | Val Acc: 0.297778 loss: 0.020202\n",
      "9\n",
      "[010/030] 104.05 sec(s) Train Acc: 0.313462 Loss: 0.018743 | Val Acc: 0.322222 loss: 0.019808\n",
      "10\n",
      "[011/030] 101.73 sec(s) Train Acc: 0.332586 Loss: 0.018092 | Val Acc: 0.344444 loss: 0.019362\n",
      "11\n",
      "[012/030] 107.61 sec(s) Train Acc: 0.349199 Loss: 0.017562 | Val Acc: 0.324444 loss: 0.020449\n",
      "12\n",
      "[013/030] 102.06 sec(s) Train Acc: 0.363206 Loss: 0.017051 | Val Acc: 0.364444 loss: 0.018313\n",
      "13\n",
      "[014/030] 106.27 sec(s) Train Acc: 0.379360 Loss: 0.016523 | Val Acc: 0.375556 loss: 0.018031\n",
      "14\n",
      "[015/030] 109.85 sec(s) Train Acc: 0.393510 Loss: 0.016151 | Val Acc: 0.426667 loss: 0.017000\n",
      "15\n",
      "[016/030] 120.82 sec(s) Train Acc: 0.408543 Loss: 0.015661 | Val Acc: 0.386667 loss: 0.017159\n",
      "16\n",
      "[017/030] 104.67 sec(s) Train Acc: 0.420955 Loss: 0.015319 | Val Acc: 0.402222 loss: 0.016537\n",
      "17\n",
      "[018/030] 104.49 sec(s) Train Acc: 0.429735 Loss: 0.014942 | Val Acc: 0.417778 loss: 0.016647\n",
      "18\n",
      "[019/030] 105.93 sec(s) Train Acc: 0.443806 Loss: 0.014622 | Val Acc: 0.426667 loss: 0.016277\n",
      "19\n",
      "[020/030] 106.49 sec(s) Train Acc: 0.453123 Loss: 0.014271 | Val Acc: 0.453333 loss: 0.015566\n",
      "20\n",
      "[021/030] 105.10 sec(s) Train Acc: 0.465788 Loss: 0.013969 | Val Acc: 0.451111 loss: 0.016065\n",
      "21\n",
      "[022/030] 104.09 sec(s) Train Acc: 0.476163 Loss: 0.013726 | Val Acc: 0.446667 loss: 0.015754\n",
      "22\n",
      "[023/030] 101.66 sec(s) Train Acc: 0.485306 Loss: 0.013473 | Val Acc: 0.451111 loss: 0.015825\n",
      "23\n",
      "[024/030] 102.22 sec(s) Train Acc: 0.490770 Loss: 0.013270 | Val Acc: 0.482222 loss: 0.014960\n",
      "24\n",
      "[025/030] 104.92 sec(s) Train Acc: 0.495934 Loss: 0.013043 | Val Acc: 0.457778 loss: 0.015517\n",
      "25\n",
      "[026/030] 104.24 sec(s) Train Acc: 0.504161 Loss: 0.012838 | Val Acc: 0.460000 loss: 0.015229\n",
      "26\n",
      "[027/030] 106.09 sec(s) Train Acc: 0.511615 Loss: 0.012595 | Val Acc: 0.471111 loss: 0.015131\n",
      "27\n",
      "[028/030] 106.94 sec(s) Train Acc: 0.517347 Loss: 0.012364 | Val Acc: 0.473333 loss: 0.015008\n",
      "28\n",
      "[029/030] 105.08 sec(s) Train Acc: 0.527169 Loss: 0.012143 | Val Acc: 0.468889 loss: 0.015027\n",
      "29\n",
      "[030/030] 108.79 sec(s) Train Acc: 0.533186 Loss: 0.011944 | Val Acc: 0.515556 loss: 0.014156\n",
      "--------Test result-------------\n",
      "Accuracy: 0.4933\n",
      "Precision: 0.5234\n",
      "Recall: 0.4933\n",
      "F1-score: 0.4930\n",
      "Loss: 0.0000\n",
      "--------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Test/Accuracy</td><td>▁</td></tr><tr><td>Test/F1-score</td><td>▁</td></tr><tr><td>Test/Loss</td><td>▁</td></tr><tr><td>Test/Precision</td><td>▁</td></tr><tr><td>Test/Recall</td><td>▁</td></tr><tr><td>Train/acc</td><td>▁▂▂▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇█████</td></tr><tr><td>Train/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>Train/loss</td><td>█▇▆▆▅▅▅▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>Val/acc</td><td>▁▁▂▂▃▃▄▄▄▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█</td></tr><tr><td>Val/epoch</td><td>▁▁▁▂▂▂▂▃▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▇▇▇▇███</td></tr><tr><td>Val/loss</td><td>█▇▇▆▅▅▅▅▄▄▃▄▃▃▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Test/Accuracy</td><td>0.49333</td></tr><tr><td>Test/F1-score</td><td>0.49304</td></tr><tr><td>Test/Loss</td><td>0.01447</td></tr><tr><td>Test/Precision</td><td>0.52339</td></tr><tr><td>Test/Recall</td><td>0.49333</td></tr><tr><td>Train/acc</td><td>0.53319</td></tr><tr><td>Train/epoch</td><td>29</td></tr><tr><td>Train/loss</td><td>0.01194</td></tr><tr><td>Val/acc</td><td>0.51556</td></tr><tr><td>Val/epoch</td><td>29</td></tr><tr><td>Val/loss</td><td>0.01416</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DynamicCNN_combination= BGR+GR+BG+R+G+B</strong> at: <a href='https://wandb.ai/sunny2021137/task1_experiment_2/runs/wmn1p6rk' target=\"_blank\">https://wandb.ai/sunny2021137/task1_experiment_2/runs/wmn1p6rk</a><br/> View project at: <a href='https://wandb.ai/sunny2021137/task1_experiment_2' target=\"_blank\">https://wandb.ai/sunny2021137/task1_experiment_2</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240617_094445-wmn1p6rk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Register count_convNd() for <class 'torch.nn.modules.conv.Conv2d'>.\n",
      "[INFO] Register count_normalization() for <class 'torch.nn.modules.batchnorm.BatchNorm2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.activation.ReLU'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.pooling.MaxPool2d'>.\n",
      "[INFO] Register zero_ops() for <class 'torch.nn.modules.container.Sequential'>.\n",
      "[INFO] Register count_linear() for <class 'torch.nn.modules.linear.Linear'>.\n",
      "FLOPs: 261.907M\n",
      "Params: 17.718M\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p76121194/dl_hw2/wandb/run-20240617_103757-zjdu3097</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sunny2021137/task1_experiment_2/runs/zjdu3097' target=\"_blank\">base_model_combination= BGR+GR+BG+R+G+B</a></strong> to <a href='https://wandb.ai/sunny2021137/task1_experiment_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sunny2021137/task1_experiment_2' target=\"_blank\">https://wandb.ai/sunny2021137/task1_experiment_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sunny2021137/task1_experiment_2/runs/zjdu3097' target=\"_blank\">https://wandb.ai/sunny2021137/task1_experiment_2/runs/zjdu3097</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[001/030] 88.91 sec(s) Train Acc: 0.034126 Loss: 0.030128 | Val Acc: 0.077778 loss: 0.030362\n",
      "1\n",
      "[002/030] 88.85 sec(s) Train Acc: 0.104161 Loss: 0.026417 | Val Acc: 0.104444 loss: 0.028780\n",
      "2\n",
      "[003/030] 85.25 sec(s) Train Acc: 0.135081 Loss: 0.025178 | Val Acc: 0.140000 loss: 0.026622\n",
      "3\n",
      "[004/030] 84.03 sec(s) Train Acc: 0.161974 Loss: 0.024221 | Val Acc: 0.168889 loss: 0.025571\n",
      "4\n",
      "[005/030] 86.72 sec(s) Train Acc: 0.186088 Loss: 0.023222 | Val Acc: 0.175556 loss: 0.024896\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# channel combination\n",
    "def experiment_1(nof_kernels):\n",
    "    # 超參數\n",
    "    #############\n",
    "    eval_time = 1\n",
    "    num_epoch = 50\n",
    "    num_classes = 50\n",
    "    img_size = 144\n",
    "    input_size = 128\n",
    "    batch_size = 128\n",
    "    lr = 0.001\n",
    "    temperature = 1\n",
    "    nof_kernels = nof_kernels\n",
    "    #############\n",
    "    set_seed(42)\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    x, y = load_img('train.txt')\n",
    "    vx, vy = load_img('val.txt')\n",
    "    tx, ty = load_img('test.txt')\n",
    "    \n",
    "    # 產生隨機組合通道數的圖片\n",
    "    channel_combinations = [['BGR','GR','BG','R','G','B'], ['BGR'], ['R','G','B'],['BG','GR'],['R', 'BG','BGR']]\n",
    "    \n",
    "    for combination in channel_combinations:\n",
    "        print(\"----channel_combinations = \", combination, end=\"----\\n\")\n",
    "        c, x_new = random_channel(x, combination)\n",
    "        vc, vx_new = random_channel(vx, combination)\n",
    "        tc, tx_new = random_channel(tx, combination)\n",
    "        \n",
    "       \n",
    "        # 填補成三通道\n",
    "        c, x_resize = resize_input(c, x_new, img_size=img_size)\n",
    "        vc, vx_resize = resize_input(vc, vx_new, img_size=img_size)\n",
    "        tc, tx_resize = resize_input(tc, tx_new, img_size=img_size)\n",
    "        del x_new, vx_new, tx_new\n",
    "        \n",
    "        # 定義transform\n",
    "        # training 時做 data augmentation\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((img_size, img_size)),  # 縮放\n",
    "            transforms.RandomRotation(degrees=30),  # 旋轉\n",
    "            transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 平移\n",
    "            transforms.RandomCrop(input_size),  # 隨機裁剪\n",
    "            transforms.RandomHorizontalFlip(),  # 水平翻轉\n",
    "            transforms.ToTensor(),  # 轉換為Tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 標準化\n",
    "\n",
    "        ])\n",
    "        # testing 時不需做 data augmentation\n",
    "        test_transform = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((img_size, img_size)),  # 縮放\n",
    "            transforms.CenterCrop(input_size),  # 中心裁剪\n",
    "            transforms.ToTensor(),  # 轉換為Tensor\n",
    "            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 標準化\n",
    "        ])\n",
    "        # data loader\n",
    "        train_set = ImgDataset(x_resize, y, train_transform)\n",
    "        val_set = ImgDataset(vx_resize, vy, test_transform)\n",
    "        test_set = ImgDataset(tx_resize, ty, test_transform)\n",
    "        train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=lambda _: np.random.seed(42), num_workers=4)\n",
    "        val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "        test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "        del x_resize, vx_resize, tx_resize\n",
    "        \n",
    "        wandb.login()\n",
    "        \n",
    "        for model_name in ['DynamicCNN', 'base_model']:\n",
    "            # create model\n",
    "            model = create_model(model_name, nof_kernels=nof_kernels).to(device)\n",
    "            # 測FLOPs、params\n",
    "            # 创建输入张量\n",
    "            inputs = torch.randn(1, 3, input_size, input_size)\n",
    "            calc_complexity(model, inputs.to(device))\n",
    "            \n",
    "            run = wandb.init(\n",
    "            # Set the project where this run will be logged\n",
    "            project=\"task1_experiment_2\",\n",
    "            name=model_name+\"_combination= \"+\"+\".join(combination),\n",
    "            config={\n",
    "                \"model\": model_name,\n",
    "                \"learning_rate\": lr,\n",
    "                \"epochs\": num_epoch,\n",
    "                \"input_size\": input_size,\n",
    "                \"nof_kernels\": nof_kernels,\n",
    "                \"channel_combination\": \"+\".join(combination)\n",
    "            },)\n",
    "            wandb.define_metric(\"Train/epoch\")\n",
    "            wandb.define_metric(\"Train/*\", step_metric=\"Train/epoch\")\n",
    "            wandb.define_metric(\"Val/epoch\")\n",
    "            wandb.define_metric(\"Val/*\", step_metric=\"Val/epoch\")\n",
    "            n_train = train_set.__len__()\n",
    "            n_val = val_set.__len__()\n",
    "            n_test = test_set.__len__()\n",
    "            train(model, train_loader, val_loader, eval_time, num_epoch, n_train, n_val, temperature, lr, device)\n",
    "            test(model, test_loader, n_test, temperature, device)\n",
    "            run.finish()\n",
    "            del model\n",
    "            gc.collect()\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "experiment_1(nof_kernels=4)   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1efc4356-5a55-4203-8cf7-3d98d78cb264",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f852fa8f-5fa2-4856-a2d2-3b169ed78e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl_hw1",
   "language": "python",
   "name": "dl_hw1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
