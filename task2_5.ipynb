{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "_9qO7OZuDXC_",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9qO7OZuDXC_",
    "outputId": "085665a1-0587-4c17-9a2a-49b5e283f319"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/gdrive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vg_KqDb3DXrW",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vg_KqDb3DXrW",
    "outputId": "afbf9d4e-63e0-4003-9cea-7e1ad6dc0952"
   },
   "outputs": [],
   "source": [
    "!unzip gdrive/MyDrive/images.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q8ICwrp8EBTM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q8ICwrp8EBTM",
    "outputId": "2e0a3568-d279-462e-8ce7-9176bc341ff4"
   },
   "outputs": [],
   "source": [
    "!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b00d1bd1-e2e0-4f71-a873-abe8c8b03b1f",
   "metadata": {
    "id": "b00d1bd1-e2e0-4f71-a873-abe8c8b03b1f"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/p76121194/miniconda3/envs/dl_hw1/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import time\n",
    "import tqdm\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "\n",
    "from torch.nn import init\n",
    "from torch.nn.modules.utils import _pair\n",
    "import math\n",
    "from timm.models.layers import to_2tuple, trunc_normal_, DropPath\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "92033f7e-66f3-4c7d-bcee-0b9feb956ea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(42)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a37727bf-f8ef-49e8-b1d1-c1b9ec5090ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAttentionBaseline(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self, n_heads, n_head_channels, n_groups,\n",
    "        attn_drop, proj_drop, stride, ksize\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "        self.n_head_channels = n_head_channels\n",
    "        self.scale = self.n_head_channels ** -0.5\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        self.nc = n_head_channels * n_heads\n",
    "        self.n_groups = n_groups\n",
    "        self.n_group_channels = self.nc // self.n_groups\n",
    "        self.ksize = ksize\n",
    "        kk = self.ksize\n",
    "        pad_size = kk // 2 if kk != stride else 0\n",
    "\n",
    "        self.conv_offset = nn.Sequential(\n",
    "            nn.Conv2d(self.n_group_channels, self.n_group_channels, kk, stride, pad_size, groups=self.n_group_channels),\n",
    "            LayerNormProxy(self.n_group_channels),\n",
    "            nn.GELU(),\n",
    "            nn.Conv2d(self.n_group_channels, 2, 1, 1, 0, bias=False)\n",
    "        )\n",
    "\n",
    "        self.proj_q = nn.Conv2d(\n",
    "            self.nc, self.nc,\n",
    "            kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "        self.proj_k = nn.Conv2d(\n",
    "            self.nc, self.nc,\n",
    "            kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "        self.proj_v = nn.Conv2d(\n",
    "            self.nc, self.nc,\n",
    "            kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "        self.proj_out = nn.Conv2d(\n",
    "            self.nc, self.nc,\n",
    "            kernel_size=1, stride=1, padding=0\n",
    "        )\n",
    "\n",
    "        self.proj_drop = nn.Dropout(proj_drop, inplace=True)\n",
    "        self.attn_drop = nn.Dropout(attn_drop, inplace=True)\n",
    "\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def _get_ref_points(self, H_key, W_key, B, dtype, device):\n",
    "\n",
    "        ref_y, ref_x = torch.meshgrid(\n",
    "            torch.linspace(0.5, H_key - 0.5, H_key, dtype=dtype, device=device),\n",
    "            torch.linspace(0.5, W_key - 0.5, W_key, dtype=dtype, device=device),\n",
    "            indexing='ij'\n",
    "        )\n",
    "        ref = torch.stack((ref_y, ref_x), -1)\n",
    "        ref[..., 1].div_(W_key - 1.0).mul_(2.0).sub_(1.0)\n",
    "        ref[..., 0].div_(H_key - 1.0).mul_(2.0).sub_(1.0)\n",
    "        ref = ref[None, ...].expand(B * self.n_groups, -1, -1, -1) # B * g H W 2\n",
    "\n",
    "        return ref\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        B, C, H, W = x.size()\n",
    "        dtype, device = x.dtype, x.device\n",
    "        # q\n",
    "        q = self.proj_q(x)\n",
    "        # offsets\n",
    "        q_off = einops.rearrange(q, 'b (g c) h w -> (b g) c h w', g=self.n_groups, c=self.n_group_channels)\n",
    "        offset = self.conv_offset(q_off).contiguous()  # B * g 2 Hg Wg\n",
    "        \n",
    "        Hk, Wk = offset.size(2), offset.size(3)\n",
    "        n_sample = Hk * Wk\n",
    "\n",
    "\n",
    "        offset = einops.rearrange(offset, 'b p h w -> b h w p')\n",
    "        reference = self._get_ref_points(Hk, Wk, B, dtype, device)\n",
    "\n",
    "        \n",
    "        \n",
    "        # 夾峙-1~1\n",
    "        if True:\n",
    "            pos = (offset + reference).clamp(-1., +1.)\n",
    "\n",
    "        \n",
    "        if True:\n",
    "            x_sampled = F.grid_sample(\n",
    "                input=x.reshape(B * self.n_groups, self.n_group_channels, H, W), \n",
    "                grid=pos[..., (1, 0)], # y, x -> x, y\n",
    "                mode='bilinear', align_corners=True) # B * g, Cg, Hg, Wg\n",
    "                \n",
    "        # x~\n",
    "        x_sampled = x_sampled.reshape(B, C, 1, n_sample)\n",
    "\n",
    "        q = q.reshape(B * self.n_heads, self.n_head_channels, H * W)\n",
    "        k = self.proj_k(x_sampled).reshape(B * self.n_heads, self.n_head_channels, n_sample)\n",
    "        v = self.proj_v(x_sampled).reshape(B * self.n_heads, self.n_head_channels, n_sample)\n",
    "\n",
    "        # att\n",
    "        attn = torch.einsum('b c m, b c n -> b m n', q, k) # B * h, HW, Ns\n",
    "        attn = attn.mul(self.scale)\n",
    "        attn = F.softmax(attn, dim=2)\n",
    "        attn = self.attn_drop(attn)\n",
    "        out = torch.einsum('b m n, b c n -> b c m', attn, v)\n",
    "\n",
    "        \n",
    "        out = out.reshape(B, C, H, W)\n",
    "        # Wo\n",
    "        y = self.proj_drop(self.proj_out(out))\n",
    "\n",
    "        return y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "046ebe15-7daf-4355-8ae9-8172d1219377",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerMLP(nn.Module):\n",
    "\n",
    "    def __init__(self, channels, expansion, drop):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.dim1 = channels\n",
    "        self.dim2 = channels * expansion\n",
    "        self.chunk = nn.Sequential()\n",
    "        self.chunk.add_module('linear1', nn.Linear(self.dim1, self.dim2))\n",
    "        self.chunk.add_module('act', nn.GELU())\n",
    "        self.chunk.add_module('drop1', nn.Dropout(drop, inplace=True))\n",
    "        self.chunk.add_module('linear2', nn.Linear(self.dim2, self.dim1))\n",
    "        self.chunk.add_module('drop2', nn.Dropout(drop, inplace=True))\n",
    "    \n",
    "    def forward(self, x):\n",
    "\n",
    "        _, _, H, W = x.size()\n",
    "        x = einops.rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.chunk(x)\n",
    "        x = einops.rearrange(x, 'b (h w) c -> b c h w', h=H, w=W)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cfce202-4dce-4106-9b67-6c6805000c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LocalAttention(nn.Module):\n",
    "\n",
    "#     def __init__(self, dim, heads, window_size, attn_drop, proj_drop):\n",
    "        \n",
    "#         super().__init__()\n",
    "\n",
    "#         window_size = to_2tuple(window_size)\n",
    "\n",
    "#         self.proj_qkv = nn.Linear(dim, 3 * dim)\n",
    "#         self.heads = heads\n",
    "#         assert dim % heads == 0\n",
    "#         head_dim = dim // heads\n",
    "#         self.scale = head_dim ** -0.5\n",
    "#         self.proj_out = nn.Linear(dim, dim)\n",
    "#         self.window_size = window_size\n",
    "#         self.proj_drop = nn.Dropout(proj_drop, inplace=True)\n",
    "#         self.attn_drop = nn.Dropout(attn_drop, inplace=True)\n",
    "\n",
    "#         Wh, Ww = self.window_size\n",
    "#         self.relative_position_bias_table = nn.Parameter(\n",
    "#             torch.zeros((2 * Wh - 1) * (2 * Ww - 1), heads)\n",
    "#         )\n",
    "#         trunc_normal_(self.relative_position_bias_table, std=0.01)\n",
    "\n",
    "#         coords_h = torch.arange(self.window_size[0])\n",
    "#         coords_w = torch.arange(self.window_size[1])\n",
    "#         coords = torch.stack(torch.meshgrid([coords_h, coords_w], indexing='ij'))  # 2, Wh, Ww\n",
    "#         coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "#         relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww\n",
    "#         relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "#         relative_coords[:, :, 0] += self.window_size[0] - 1  # shift to start from 0\n",
    "#         relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "#         relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "#         relative_position_index = relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "#         self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "\n",
    "#     def forward(self, x, mask=None):\n",
    "\n",
    "#         B, C, H, W = x.size()\n",
    "#         r1, r2 = H // self.window_size[0], W // self.window_size[1]\n",
    "\n",
    "#         x_total = einops.rearrange(x, 'b c (r1 h1) (r2 w1) -> b (r1 r2) (h1 w1) c', h1=self.window_size[0], w1=self.window_size[1]) # B x Nr x Ws x C\n",
    "\n",
    "#         x_total = einops.rearrange(x_total, 'b m n c -> (b m) n c')\n",
    "\n",
    "#         qkv = self.proj_qkv(x_total) # B' x N x 3C\n",
    "#         q, k, v = torch.chunk(qkv, 3, dim=2)\n",
    "\n",
    "#         q = q * self.scale\n",
    "#         q, k, v = [einops.rearrange(t, 'b n (h c1) -> b h n c1', h=self.heads) for t in [q, k, v]]\n",
    "#         attn = torch.einsum('b h m c, b h n c -> b h m n', q, k)\n",
    "\n",
    "#         relative_position_bias = self.relative_position_bias_table[self.relative_position_index.view(-1)].view(\n",
    "#             self.window_size[0] * self.window_size[1], self.window_size[0] * self.window_size[1], -1)  # Wh*Ww,Wh*Ww,nH\n",
    "#         relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "#         attn_bias = relative_position_bias\n",
    "#         attn = attn + attn_bias.unsqueeze(0)\n",
    "\n",
    "#         if mask is not None:\n",
    "#             # attn : (b * nW) h w w\n",
    "#             # mask : nW ww ww\n",
    "#             nW, ww, _ = mask.size()\n",
    "#             attn = einops.rearrange(attn, '(b n) h w1 w2 -> b n h w1 w2', n=nW, h=self.heads, w1=ww, w2=ww) + mask.reshape(1, nW, 1, ww, ww)\n",
    "#             attn = einops.rearrange(attn, 'b n h w1 w2 -> (b n) h w1 w2')\n",
    "#         attn = self.attn_drop(attn.softmax(dim=3))\n",
    "\n",
    "#         x = torch.einsum('b h m n, b h n c -> b h m c', attn, v)\n",
    "#         x = einops.rearrange(x, 'b h n c1 -> b n (h c1)')\n",
    "#         x = self.proj_drop(self.proj_out(x)) # B' x N x C\n",
    "#         x = einops.rearrange(x, '(b r1 r2) (h1 w1) c -> b c (r1 h1) (r2 w1)', r1=r1, r2=r2, h1=self.window_size[0], w1=self.window_size[1]) # B x C x H x W\n",
    "\n",
    "#         return x, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c2d0f35-a471-4c0b-80a0-9956e7d8bb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerStage(nn.Module):\n",
    "\n",
    "    def __init__(self, fmap_size, window_size,\n",
    "                 dim_in, dim_embed, depths, stage_spec, n_groups, \n",
    "                 heads, stride,\n",
    "                 attn_drop, proj_drop, expansion, drop,\n",
    "                 use_dwc_mlp, ksize):\n",
    "\n",
    "        super().__init__()\n",
    "        fmap_size = to_2tuple(fmap_size)\n",
    "        # depths：幾層att\n",
    "        self.depths = depths\n",
    "        # embedding dim要能整除head\n",
    "        hc = dim_embed // heads\n",
    "        assert dim_embed == heads * hc\n",
    "\n",
    "        # 轉換成embedding dim\n",
    "        self.proj = nn.Conv2d(dim_in, dim_embed, 1, 1, 0) if dim_in != dim_embed else nn.Identity()\n",
    "        # 使用甚麼layer\n",
    "        self.stage_spec = stage_spec\n",
    "\n",
    "        \n",
    "        self.layer_norms = nn.ModuleList(\n",
    "            [LayerNormProxy(dim_embed)  for d in range(2 * depths)]\n",
    "        )\n",
    "\n",
    "        mlp_fn = TransformerMLP\n",
    "\n",
    "        self.mlps = nn.ModuleList(\n",
    "            [ \n",
    "                mlp_fn(dim_embed, expansion, drop) for _ in range(depths)\n",
    "            ]\n",
    "        )\n",
    "        self.attns = nn.ModuleList()\n",
    "        self.drop_path = nn.ModuleList()\n",
    "        \n",
    "\n",
    "        for i in range(depths):\n",
    "            if stage_spec[i] == 'L':\n",
    "                self.attns.append(\n",
    "                    LocalAttention(dim_embed, heads, window_size, attn_drop, proj_drop)\n",
    "                )\n",
    "            elif stage_spec[i] == 'D':\n",
    "                self.attns.append(\n",
    "                    DAttentionBaseline(heads, \n",
    "                    hc, n_groups, attn_drop, proj_drop, \n",
    "                    stride, ksize)\n",
    "                )\n",
    "            elif stage_spec[i] == 'S':\n",
    "                shift_size = math.ceil(window_size / 2)\n",
    "                self.attns.append(\n",
    "                    ShiftWindowAttention(dim_embed, heads, window_size, attn_drop, proj_drop, shift_size, fmap_size)\n",
    "                )\n",
    "            else:\n",
    "                raise NotImplementedError(f'Spec: {stage_spec[i]} is not supported.')\n",
    "        # self.drop_path.append(DropPath(drop_path_rate[i]) if drop_path_rate[i] > 0.0 else nn.Identity())         \n",
    "\n",
    "    def forward(self, x):\n",
    "        # 轉換成embedding dim\n",
    "        x = self.proj(x)\n",
    "        # transformer block\n",
    "        for d in range(self.depths):\n",
    "            if True:\n",
    "                x0 = x\n",
    "                # attention\n",
    "                x = self.attns[d](self.layer_norms[2 * d](x))\n",
    "                # x = self.drop_path[d](x) + x0\n",
    "                x0 = x\n",
    "                x = self.mlps[d](self.layer_norms[2 * d + 1](x))\n",
    "                # x = self.drop_path[d](x) + x0\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74b226a1-1067-4780-af7d-654a2d474fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormProxy(nn.Module):\n",
    "    \n",
    "    def __init__(self, dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = einops.rearrange(x, 'b c h w -> b h w c')\n",
    "        x = self.norm(x)\n",
    "        return einops.rearrange(x, 'b h w c -> b c h w')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59559eed-3dbe-4f0b-bfe0-e0968569a6e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# group要能被channel整除\n",
    "class DAT(nn.Module):\n",
    "\n",
    "    def __init__(self, img_size=128, patch_size=4, num_classes=50, expansion=4,\n",
    "                 dim_stem=64, dims=[64, 128, 256, 512], depths=[1, 1, 1, 1], \n",
    "                 heads=[2, 4, 8, 16],\n",
    "                 window_sizes=[4, 4, 4, 4],\n",
    "                 drop_rate=0.0, attn_drop_rate=0.0,\n",
    "                 strides=[8, 4, 2, 1],\n",
    "                 stage_spec=[['D'], ['D'], ['D'], ['D']], \n",
    "                 groups=[1, 2, 4, 8],\n",
    "                 use_dwc_mlps=[False, False, False, False],\n",
    "                 ksizes=[9, 7, 5, 3],\n",
    "                 **kwargs):\n",
    "        \n",
    "        super().__init__()\n",
    "        # dim_stem = C\n",
    "        self.patch_proj = nn.Sequential(\n",
    "            nn.Conv2d(3, dim_stem, patch_size, patch_size, 0),\n",
    "            # channel norm\n",
    "            LayerNormProxy(dim_stem)\n",
    "        )\n",
    "        # 新spatial size\n",
    "        img_size = img_size // patch_size\n",
    "        \n",
    "\n",
    "        self.stages = nn.ModuleList()\n",
    "        for i in range(4):\n",
    "            # 各層的dim\n",
    "            dim1 = dim_stem if i == 0 else dims[i - 1] * 2\n",
    "            dim2 = dims[i]\n",
    "            self.stages.append(\n",
    "                \n",
    "                TransformerStage(\n",
    "                    img_size, window_sizes[i],\n",
    "                    dim1, dim2, depths[i],\n",
    "                    stage_spec[i], groups[i], heads[i], strides[i],\n",
    "                    \n",
    "                    attn_drop_rate, drop_rate, expansion, drop_rate, use_dwc_mlps[i],\n",
    "                    ksizes[i]\n",
    "                )\n",
    "            )\n",
    "            # 每層後spatial size/=2\n",
    "            img_size = img_size // 2\n",
    "\n",
    "        self.down_projs = nn.ModuleList()\n",
    "        for i in range(3):\n",
    "            self.down_projs.append(\n",
    "            # stride為2的醬採樣\n",
    "               nn.Sequential(\n",
    "                    nn.Conv2d(dims[i], dims[i + 1], 2, 2, 0, bias=False),\n",
    "                    LayerNormProxy(dims[i + 1])\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        self.cls_norm = LayerNormProxy(dims[-1]) \n",
    "        self.cls_head = nn.Linear(dims[-1], num_classes)\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "\n",
    "        for m in self.parameters():\n",
    "            if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # patch embedding\n",
    "        x = self.patch_proj(x)\n",
    "        # 4 stage\n",
    "        for i in range(4):\n",
    "            x = self.stages[i](x)\n",
    "            # down\n",
    "            if i < 3:\n",
    "                x = self.down_projs[i](x)\n",
    "        # normalize\n",
    "        x = self.cls_norm(x)\n",
    "        # pool\n",
    "        x = F.adaptive_avg_pool2d(x, 1)\n",
    "        # classifier\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.cls_head(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f32bac3a-b23a-4a9e-ab9c-96edcc39ca0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1), # [32, 128, 128]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [32, 64, 64]\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), # [64, 64, 64]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 32, 32]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 32, 32]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 16, 16]\n",
    "\n",
    "            nn.Conv2d(128, 256, 3, 1, 1), # [256, 16, 16]\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),       # [256, 8, 8]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(256*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = out.view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9743e915-3b0b-414b-8bfc-d89334a8cae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(DATCNN, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1), # [32, 128, 128]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [32, 64, 64]\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), # [64, 64, 64]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 32, 32]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 32, 32]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 16, 16]\n",
    "\n",
    "            # nn.Conv2d(128, 256, 3, 1, 1), # [256, 16, 16]\n",
    "            # nn.BatchNorm2d(256),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool2d(2, 2, 0),       # [256, 8, 8]\n",
    "        )\n",
    "        n_heads = 4\n",
    "        self.dat = DAttentionBaseline(n_heads=n_heads, n_head_channels=128//n_heads, n_groups=2, attn_drop=0, proj_drop=0, stride=1, ksize=3)\n",
    "        self.nn2 = nn.Sequential(\n",
    "            LayerNormProxy(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = self.dat(out)\n",
    "        out = self.nn2(out)\n",
    "        out = out.contiguous().view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "aa0efb4e-7084-44a6-b41c-db63c48f0887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATCNN(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(DATCNN, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, 3, 1, 1), # [32, 128, 128]\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [32, 64, 64]\n",
    "\n",
    "            nn.Conv2d(32, 64, 3, 1, 1), # [64, 64, 64]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 32, 32]\n",
    "\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 32, 32]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 16, 16]\n",
    "\n",
    "            # nn.Conv2d(128, 256, 3, 1, 1), # [256, 16, 16]\n",
    "            # nn.BatchNorm2d(256),\n",
    "            # nn.ReLU(),\n",
    "            # nn.MaxPool2d(2, 2, 0),       # [256, 8, 8]\n",
    "        )\n",
    "        n_heads = 4\n",
    "        self.dat = DAttentionBaseline(n_heads=n_heads, n_head_channels=128//n_heads, n_groups=2, attn_drop=0, proj_drop=0, stride=1, ksize=3)\n",
    "        self.norm = LayerNormProxy(128)\n",
    "        self.nn2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn(x)\n",
    "        out = self.norm(self.dat(out)) + out\n",
    "        out = self.nn2(out)\n",
    "        out = out.contiguous().view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1d23ee9e-3c6b-4169-8e8b-a814be593556",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATCNN_2(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(DATCNN_2, self).__init__()\n",
    "        \n",
    "        self.cnn_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), # [64, 128, 128]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
    "        )\n",
    "        \n",
    "        self.cnn_2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 32, 32]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 16, 16]\n",
    "        )\n",
    "        n_heads = 2\n",
    "        self.dat_1 = nn.Sequential(\n",
    "            DAttentionBaseline(n_heads=n_heads, n_head_channels=64//n_heads, n_groups=1, attn_drop=0, proj_drop=0, stride=4, ksize=7),# [64, 64, 64]\n",
    "            LayerNormProxy(64),\n",
    "        )\n",
    "        self.down_1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),# [64, 32, 32]\n",
    "        )\n",
    "        n_heads = 4\n",
    "        self.dat_2 = nn.Sequential(\n",
    "            DAttentionBaseline(n_heads=n_heads, n_head_channels=128//n_heads, n_groups=2, attn_drop=0, proj_drop=0, stride=1, ksize=3),# [128, 16, 16]\n",
    "            LayerNormProxy(128)\n",
    "        )\n",
    "        self.down_2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),# [128, 8, 8]\n",
    "        )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn_1(x)\n",
    "        out = self.down_1(self.dat_1(out)+out)\n",
    "        out = self.cnn_2(out)\n",
    "        out = self.down_2(self.dat_2(out)+out)\n",
    "        out = out.contiguous().view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "8955f7d1-861c-4204-944d-e06a03d9b5db",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATCNN_3(nn.Module):\n",
    "    def __init__(self, num_classes=50):\n",
    "        super(DATCNN_3, self).__init__()\n",
    "        \n",
    "        self.cnn_1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, 1, 1), # [64, 128, 128]\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [64, 64, 64]\n",
    "        )\n",
    "        \n",
    "        self.cnn_2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, 1, 1), # [128, 64, 64]\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),      # [128, 32, 32]\n",
    "        )\n",
    "        n_heads = 4\n",
    "        self.dat_1 = nn.Sequential(\n",
    "            DAttentionBaseline(n_heads=n_heads, n_head_channels=128//n_heads, n_groups=4, attn_drop=0, proj_drop=0, stride=2, ksize=5),# [128, 32, 32]\n",
    "            LayerNormProxy(128)\n",
    "        )\n",
    "        self.down_1 = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),# [128, 16, 16]\n",
    "        )\n",
    "        n_heads = 8\n",
    "        self.dat_2 = nn.Sequential(\n",
    "            DAttentionBaseline(n_heads=n_heads, n_head_channels=128//n_heads, n_groups=8, attn_drop=0, proj_drop=0, stride=1, ksize=3),# [128, 16, 16]\n",
    "            LayerNormProxy(128)\n",
    "        )\n",
    "        self.down_2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2, 0),# [128, 8, 8]\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(128*8*8, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.cnn_1(x)\n",
    "        out = self.cnn_2(out)\n",
    "        out = self.down_1(self.dat_1(out)+out)\n",
    "        out = self.down_2(self.dat_2(out)+out)\n",
    "        out = out.contiguous().view(out.size()[0], -1)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1411f880-1ba5-4590-8b0e-2664d08b8d04",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "274ca17c-2a38-4461-b261-cdcb8c1d161e",
   "metadata": {
    "id": "274ca17c-2a38-4461-b261-cdcb8c1d161e"
   },
   "outputs": [],
   "source": [
    "def load_img(f):\n",
    "    shapes = []\n",
    "    f=open(f)\n",
    "    lines=f.readlines()\n",
    "    imgs, lab=[], []\n",
    "    for i in range(len(lines)):\n",
    "        fn, label = lines[i].split(' ')\n",
    "        im1=cv2.imread(fn)\n",
    "\n",
    "        if im1.shape[2] not in shapes:\n",
    "            shapes.append(im1.shape[2])\n",
    "        # im1=cv2.resize(im1, (img_size,img_size))\n",
    "        # im1 = cv2.cvtColor(im1, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # im1 = preprocessing(im1, op_list)\n",
    "        # vec = np.reshape(im1, [-1])\n",
    "\n",
    "        imgs.append(im1)\n",
    "        lab.append(int(label))\n",
    "    print(i)\n",
    "\n",
    "    # imgs= np.asarray(imgs, np.uint8)\n",
    "    lab= np.asarray(lab, np.uint8)\n",
    "    # print(shapes)\n",
    "    return imgs, lab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "_hI4HXjfHRKF",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "id": "_hI4HXjfHRKF",
    "outputId": "c9914f4d-1b62-45a5-d042-97da637119eb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msunny2021137\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fd392f61-9289-4fe8-a30a-c037c2fb947a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "fd392f61-9289-4fe8-a30a-c037c2fb947a",
    "outputId": "f1acf56e-2a64-4db3-84aa-5cb89be3df5c"
   },
   "outputs": [],
   "source": [
    "\n",
    "    #############\n",
    "    eval_time = 1\n",
    "    num_epoch = 30\n",
    "    num_classes = 50\n",
    "    img_size = 144\n",
    "    batch_size = 128\n",
    "    lr = 0.001\n",
    "    \n",
    "\n",
    "    # BATCH_SIZE = 64\n",
    "    # EPOCHS = 10\n",
    "    # LEARNING_RATE = 0.01\n",
    "    \n",
    "    img_shape = [3,128,128]\n",
    "    patch_size = [16,16]\n",
    "    hidden_dim = 8\n",
    "    num_heads = 3\n",
    "\n",
    "    #############\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aNhJvgvAI1SQ",
   "metadata": {
    "id": "aNhJvgvAI1SQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63324\n",
      "449\n",
      "449\n"
     ]
    }
   ],
   "source": [
    "    x, y = load_img('train.txt')\n",
    "    vx, vy = load_img('val.txt')\n",
    "    tx, ty = load_img('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5097372-b883-4461-8c9f-ea518fdb7ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--3--\n"
     ]
    }
   ],
   "source": [
    "    print(\"--3--\")\n",
    "    # training 時做 data augmentation\n",
    "    train_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((144, 144)),  # 縮放\n",
    "        transforms.RandomRotation(degrees=30),  # 旋轉\n",
    "        transforms.RandomAffine(degrees=0, translate=(0.1, 0.1)),  # 平移\n",
    "        transforms.RandomCrop(128),  # 隨機裁剪\n",
    "        transforms.RandomHorizontalFlip(),  # 水平翻轉\n",
    "        transforms.ToTensor(),  # 轉換為Tensor\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 標準化\n",
    "\n",
    "    ])\n",
    "    # testing 時不需做 data augmentation\n",
    "    test_transform = transforms.Compose([\n",
    "        transforms.ToPILImage(),\n",
    "        transforms.Resize((144, 144)),  # 縮放\n",
    "        transforms.CenterCrop(128),  # 中心裁剪\n",
    "        transforms.ToTensor(),  # 轉換為Tensor\n",
    "        # transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # 標準化\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32f40359-f0d9-43d0-b155-b9e1982b0906",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "\n",
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, x, y=None, transform=None):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        if y is not None:\n",
    "            self.y = torch.LongTensor(y)\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    def __getitem__(self, index):\n",
    "        X = self.x[index]\n",
    "        if self.transform is not None:\n",
    "            X = self.transform(X)\n",
    "        if self.y is not None:\n",
    "            Y = self.y[index]\n",
    "            return X, Y\n",
    "        else:\n",
    "            return X\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "R7rzfzd_GlSl",
   "metadata": {
    "id": "R7rzfzd_GlSl"
   },
   "outputs": [],
   "source": [
    "    train_set = ImgDataset(x, y, train_transform)\n",
    "    val_set = ImgDataset(vx, vy, test_transform)\n",
    "    test_set = ImgDataset(tx, ty, test_transform)\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, worker_init_fn=lambda _: np.random.seed(42))\n",
    "    val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6506b2fa-0a04-40fe-ae8e-939217199dff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.1 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/p76121194/dl_hw2/wandb/run-20240613_223642-he4jqn8m</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/sunny2021137/task2_2/runs/he4jqn8m' target=\"_blank\">prime-bee-26</a></strong> to <a href='https://wandb.ai/sunny2021137/task2_2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/sunny2021137/task2_2' target=\"_blank\">https://wandb.ai/sunny2021137/task2_2</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/sunny2021137/task2_2/runs/he4jqn8m' target=\"_blank\">https://wandb.ai/sunny2021137/task2_2/runs/he4jqn8m</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<wandb.sdk.wandb_metric.Metric at 0x7f6c40fb11f0>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    run = wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"task2_2\",\n",
    "    # Track hyperparameters and run metadata\n",
    "    config={\n",
    "        \"model\": \"dat\",\n",
    "        \"learning_rate\": lr,\n",
    "        \"epochs\": num_epoch,\n",
    "        \"img_size\": img_size,\n",
    "    },)\n",
    "    wandb.define_metric(\"Train/epoch\")\n",
    "    wandb.define_metric(\"Train/*\", step_metric=\"Train/epoch\")\n",
    "    wandb.define_metric(\"Val/epoch\")\n",
    "    wandb.define_metric(\"Val/*\", step_metric=\"Val/epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0SPBb0ciGqa_",
   "metadata": {
    "id": "0SPBb0ciGqa_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # print(\"--4--\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = torch.device(\"cpu\")\n",
    "    model = DATCNN_3(num_classes=50).to(device)\n",
    "\n",
    "    loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) # optimizer 使用 Adam\n",
    "\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        print(epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "            train_pred = model(data[0].to(device)) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "            batch_loss = loss(train_pred, data[1].to(device)) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "            batch_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
    "            optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "            train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "        if True:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    val_pred = model(data[0].to(device))\n",
    "                    batch_loss = loss(val_pred, data[1].to(device))\n",
    "\n",
    "                    val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                \n",
    "                wandb.log({\"Train/epoch\": epoch,\n",
    "                            \"Train/acc\": train_acc/train_set.__len__(),\n",
    "                           \"Train/loss\": train_loss/train_set.__len__(),\n",
    "                           \"Val/epoch\": epoch,\n",
    "                           \"Val/acc\": val_acc/val_set.__len__(),\n",
    "                           \"Val/loss\": val_loss/val_set.__len__(),\n",
    "                          })\n",
    "\n",
    "    print(\"--5--\")\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            test_pred = model(data[0].to(device))\n",
    "            batch_loss = loss(test_pred, data[1].to(device))\n",
    "            test_acc += np.sum(np.argmax(test_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            test_loss += batch_loss.item()\n",
    "        wandb.log({\"Test/test acc\": test_acc/test_set.__len__(),\n",
    "                  \"Test/loss\": test_loss/test_set.__len__(),})\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ced169-ab3f-4f69-a311-fb110a41ede5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c68a3-6276-4e10-91a8-f00a516c1950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    }
   ],
   "source": [
    "    import torchvision.models as models\n",
    "    # print(\"--4--\")\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "    # device = torch.device(\"cpu\")\n",
    "    # 加载 ResNet34 模型\n",
    "    model = models.resnet34()\n",
    "    num_ftrs = model.fc.in_features\n",
    "    model.fc = nn.Linear(num_ftrs, num_classes)  # 设置最后一层输出为分类数目\n",
    "    model.to(device)\n",
    "    loss = nn.CrossEntropyLoss() # 因為是 classification task，所以 loss 使用 CrossEntropyLoss\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr) # optimizer 使用 Adam\n",
    "\n",
    "\n",
    "    for epoch in range(num_epoch):\n",
    "        print(epoch)\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc = 0.0\n",
    "        train_loss = 0.0\n",
    "        val_acc = 0.0\n",
    "        val_loss = 0.0\n",
    "\n",
    "        model.train() # 確保 model 是在 train model (開啟 Dropout 等...)\n",
    "        for i, data in enumerate(train_loader):\n",
    "            optimizer.zero_grad() # 用 optimizer 將 model 參數的 gradient 歸零\n",
    "            train_pred = model(data[0].to(device)) # 利用 model 得到預測的機率分佈 這邊實際上就是去呼叫 model 的 forward 函數\n",
    "            batch_loss = loss(train_pred, data[1].to(device)) # 計算 loss （注意 prediction 跟 label 必須同時在 CPU 或是 GPU 上）\n",
    "            batch_loss.backward() # 利用 back propagation 算出每個參數的 gradient\n",
    "            optimizer.step() # 以 optimizer 用 gradient 更新參數值\n",
    "\n",
    "            train_acc += np.sum(np.argmax(train_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            train_loss += batch_loss.item()\n",
    "\n",
    "        if True:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                for i, data in enumerate(val_loader):\n",
    "                    val_pred = model(data[0].to(device))\n",
    "                    batch_loss = loss(val_pred, data[1].to(device))\n",
    "\n",
    "                    val_acc += np.sum(np.argmax(val_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "                    val_loss += batch_loss.item()\n",
    "\n",
    "                \n",
    "                wandb.log({\"Train/epoch\": epoch,\n",
    "                            \"Train/acc\": train_acc/train_set.__len__(),\n",
    "                           \"Train/loss\": train_loss/train_set.__len__(),\n",
    "                           \"Val/epoch\": epoch,\n",
    "                           \"Val/acc\": val_acc/val_set.__len__(),\n",
    "                           \"Val/loss\": val_loss/val_set.__len__(),\n",
    "                          })\n",
    "\n",
    "    print(\"--5--\")\n",
    "    model.eval()\n",
    "    test_acc = 0.0\n",
    "    test_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            test_pred = model(data[0].to(device))\n",
    "            batch_loss = loss(test_pred, data[1].to(device))\n",
    "            test_acc += np.sum(np.argmax(test_pred.cpu().data.numpy(), axis=1) == data[1].numpy())\n",
    "            test_loss += batch_loss.item()\n",
    "        wandb.log({\"Test/test acc\": test_acc/test_set.__len__(),\n",
    "                  \"Test/loss\": test_loss/test_set.__len__(),})\n",
    "    run.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8472cc50-0195-4286-b540-6dad3cd2aab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "del model\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f009ab3d-721f-49b1-a82f-8ec5b78fad75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575677b3-96a3-40fc-8c72-7ff9977f1033",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9391925f-0521-4d18-b97e-c5e4da08e6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dl_hw1",
   "language": "python",
   "name": "dl_hw1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
